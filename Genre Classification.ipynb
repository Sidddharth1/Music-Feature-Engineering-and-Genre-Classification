{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "from xgboost import XGBClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Beats_Mean</th>\n",
       "      <th>Beats_SD</th>\n",
       "      <th>Beats_1</th>\n",
       "      <th>Beats_1_SD</th>\n",
       "      <th>Beats_2</th>\n",
       "      <th>Beats_2_SD</th>\n",
       "      <th>...</th>\n",
       "      <th>STFT_12_1</th>\n",
       "      <th>STFT_12_1_SD</th>\n",
       "      <th>STFT_12_2</th>\n",
       "      <th>STFT_12_2_SD</th>\n",
       "      <th>STFT_12_3</th>\n",
       "      <th>STFT_12_3_SD</th>\n",
       "      <th>STFT_12_4</th>\n",
       "      <th>STFT_12_4_SD</th>\n",
       "      <th>STFT_12_5</th>\n",
       "      <th>STFT_12_5_SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>blues.00000</td>\n",
       "      <td>blues</td>\n",
       "      <td>123.046875</td>\n",
       "      <td>631.098361</td>\n",
       "      <td>363.273040</td>\n",
       "      <td>135.307692</td>\n",
       "      <td>77.485864</td>\n",
       "      <td>394.500000</td>\n",
       "      <td>71.721103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360912</td>\n",
       "      <td>0.306167</td>\n",
       "      <td>0.394035</td>\n",
       "      <td>0.274264</td>\n",
       "      <td>0.371622</td>\n",
       "      <td>0.271165</td>\n",
       "      <td>0.337571</td>\n",
       "      <td>0.256537</td>\n",
       "      <td>0.460110</td>\n",
       "      <td>0.332735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>blues.00001</td>\n",
       "      <td>blues</td>\n",
       "      <td>67.999589</td>\n",
       "      <td>626.941176</td>\n",
       "      <td>370.204470</td>\n",
       "      <td>117.571429</td>\n",
       "      <td>76.144331</td>\n",
       "      <td>380.142857</td>\n",
       "      <td>76.006444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483023</td>\n",
       "      <td>0.270392</td>\n",
       "      <td>0.294946</td>\n",
       "      <td>0.240578</td>\n",
       "      <td>0.228129</td>\n",
       "      <td>0.245542</td>\n",
       "      <td>0.279131</td>\n",
       "      <td>0.256822</td>\n",
       "      <td>0.385768</td>\n",
       "      <td>0.253855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>blues.00002</td>\n",
       "      <td>blues</td>\n",
       "      <td>161.499023</td>\n",
       "      <td>638.727273</td>\n",
       "      <td>364.188214</td>\n",
       "      <td>137.437500</td>\n",
       "      <td>76.151140</td>\n",
       "      <td>401.437500</td>\n",
       "      <td>75.893815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536629</td>\n",
       "      <td>0.339453</td>\n",
       "      <td>0.502290</td>\n",
       "      <td>0.345287</td>\n",
       "      <td>0.686320</td>\n",
       "      <td>0.350614</td>\n",
       "      <td>0.619982</td>\n",
       "      <td>0.330648</td>\n",
       "      <td>0.597478</td>\n",
       "      <td>0.368183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>blues.00003</td>\n",
       "      <td>blues</td>\n",
       "      <td>63.024009</td>\n",
       "      <td>588.892857</td>\n",
       "      <td>334.657750</td>\n",
       "      <td>132.666667</td>\n",
       "      <td>71.247612</td>\n",
       "      <td>381.833333</td>\n",
       "      <td>70.706946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348773</td>\n",
       "      <td>0.343449</td>\n",
       "      <td>0.406141</td>\n",
       "      <td>0.342834</td>\n",
       "      <td>0.424358</td>\n",
       "      <td>0.360890</td>\n",
       "      <td>0.386342</td>\n",
       "      <td>0.327595</td>\n",
       "      <td>0.349465</td>\n",
       "      <td>0.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>blues.00004</td>\n",
       "      <td>blues</td>\n",
       "      <td>135.999178</td>\n",
       "      <td>640.136364</td>\n",
       "      <td>369.197774</td>\n",
       "      <td>134.571429</td>\n",
       "      <td>78.570000</td>\n",
       "      <td>399.230769</td>\n",
       "      <td>72.615059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247868</td>\n",
       "      <td>0.214111</td>\n",
       "      <td>0.307350</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.279017</td>\n",
       "      <td>0.209529</td>\n",
       "      <td>0.300079</td>\n",
       "      <td>0.211344</td>\n",
       "      <td>0.230249</td>\n",
       "      <td>0.187255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 460 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Song Name  Genre       Tempo  Beats_Mean    Beats_SD  \\\n",
       "0           0  blues.00000  blues  123.046875  631.098361  363.273040   \n",
       "1           1  blues.00001  blues   67.999589  626.941176  370.204470   \n",
       "2           2  blues.00002  blues  161.499023  638.727273  364.188214   \n",
       "3           3  blues.00003  blues   63.024009  588.892857  334.657750   \n",
       "4           4  blues.00004  blues  135.999178  640.136364  369.197774   \n",
       "\n",
       "      Beats_1  Beats_1_SD     Beats_2  Beats_2_SD  ...  STFT_12_1  \\\n",
       "0  135.307692   77.485864  394.500000   71.721103  ...   0.360912   \n",
       "1  117.571429   76.144331  380.142857   76.006444  ...   0.483023   \n",
       "2  137.437500   76.151140  401.437500   75.893815  ...   0.536629   \n",
       "3  132.666667   71.247612  381.833333   70.706946  ...   0.348773   \n",
       "4  134.571429   78.570000  399.230769   72.615059  ...   0.247868   \n",
       "\n",
       "   STFT_12_1_SD  STFT_12_2  STFT_12_2_SD  STFT_12_3  STFT_12_3_SD  STFT_12_4  \\\n",
       "0      0.306167   0.394035      0.274264   0.371622      0.271165   0.337571   \n",
       "1      0.270392   0.294946      0.240578   0.228129      0.245542   0.279131   \n",
       "2      0.339453   0.502290      0.345287   0.686320      0.350614   0.619982   \n",
       "3      0.343449   0.406141      0.342834   0.424358      0.360890   0.386342   \n",
       "4      0.214111   0.307350      0.253906   0.279017      0.209529   0.300079   \n",
       "\n",
       "   STFT_12_4_SD  STFT_12_5  STFT_12_5_SD  \n",
       "0      0.256537   0.460110      0.332735  \n",
       "1      0.256822   0.385768      0.253855  \n",
       "2      0.330648   0.597478      0.368183  \n",
       "3      0.327595   0.349465      0.303100  \n",
       "4      0.211344   0.230249      0.187255  \n",
       "\n",
       "[5 rows x 460 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Beats_Mean</th>\n",
       "      <th>Beats_SD</th>\n",
       "      <th>Beats_1</th>\n",
       "      <th>Beats_1_SD</th>\n",
       "      <th>Beats_2</th>\n",
       "      <th>Beats_2_SD</th>\n",
       "      <th>...</th>\n",
       "      <th>STFT_12_1</th>\n",
       "      <th>STFT_12_1_SD</th>\n",
       "      <th>STFT_12_2</th>\n",
       "      <th>STFT_12_2_SD</th>\n",
       "      <th>STFT_12_3</th>\n",
       "      <th>STFT_12_3_SD</th>\n",
       "      <th>STFT_12_4</th>\n",
       "      <th>STFT_12_4_SD</th>\n",
       "      <th>STFT_12_5</th>\n",
       "      <th>STFT_12_5_SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>293</td>\n",
       "      <td>country.00093</td>\n",
       "      <td>country</td>\n",
       "      <td>86.132812</td>\n",
       "      <td>619.595238</td>\n",
       "      <td>365.842996</td>\n",
       "      <td>121.111111</td>\n",
       "      <td>73.283990</td>\n",
       "      <td>394.111111</td>\n",
       "      <td>77.730938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149727</td>\n",
       "      <td>0.209647</td>\n",
       "      <td>0.257258</td>\n",
       "      <td>0.361308</td>\n",
       "      <td>0.206903</td>\n",
       "      <td>0.299516</td>\n",
       "      <td>0.203994</td>\n",
       "      <td>0.301832</td>\n",
       "      <td>0.205054</td>\n",
       "      <td>0.270712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>978</td>\n",
       "      <td>rock.00078</td>\n",
       "      <td>rock</td>\n",
       "      <td>99.384014</td>\n",
       "      <td>593.888889</td>\n",
       "      <td>342.603186</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>67.993464</td>\n",
       "      <td>356.444444</td>\n",
       "      <td>67.906981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498799</td>\n",
       "      <td>0.322212</td>\n",
       "      <td>0.299644</td>\n",
       "      <td>0.252775</td>\n",
       "      <td>0.228645</td>\n",
       "      <td>0.208432</td>\n",
       "      <td>0.335407</td>\n",
       "      <td>0.253260</td>\n",
       "      <td>0.277329</td>\n",
       "      <td>0.229559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>132</td>\n",
       "      <td>classical.00032</td>\n",
       "      <td>classical</td>\n",
       "      <td>103.359375</td>\n",
       "      <td>634.437500</td>\n",
       "      <td>346.060622</td>\n",
       "      <td>157.100000</td>\n",
       "      <td>71.568778</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>75.288777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140701</td>\n",
       "      <td>0.189802</td>\n",
       "      <td>0.135167</td>\n",
       "      <td>0.139561</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>0.098088</td>\n",
       "      <td>0.153734</td>\n",
       "      <td>0.173352</td>\n",
       "      <td>0.176123</td>\n",
       "      <td>0.133869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>887</td>\n",
       "      <td>reggae.00087</td>\n",
       "      <td>reggae</td>\n",
       "      <td>83.354335</td>\n",
       "      <td>618.230769</td>\n",
       "      <td>349.762004</td>\n",
       "      <td>136.250000</td>\n",
       "      <td>71.140266</td>\n",
       "      <td>385.500000</td>\n",
       "      <td>71.029923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515735</td>\n",
       "      <td>0.305537</td>\n",
       "      <td>0.537594</td>\n",
       "      <td>0.351125</td>\n",
       "      <td>0.464829</td>\n",
       "      <td>0.292321</td>\n",
       "      <td>0.407135</td>\n",
       "      <td>0.322346</td>\n",
       "      <td>0.430699</td>\n",
       "      <td>0.315258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>567</td>\n",
       "      <td>jazz.00067</td>\n",
       "      <td>jazz</td>\n",
       "      <td>99.384014</td>\n",
       "      <td>632.122449</td>\n",
       "      <td>363.047342</td>\n",
       "      <td>131.700000</td>\n",
       "      <td>74.037896</td>\n",
       "      <td>387.900000</td>\n",
       "      <td>73.305457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111597</td>\n",
       "      <td>0.188841</td>\n",
       "      <td>0.345629</td>\n",
       "      <td>0.372253</td>\n",
       "      <td>0.316785</td>\n",
       "      <td>0.322037</td>\n",
       "      <td>0.183546</td>\n",
       "      <td>0.258258</td>\n",
       "      <td>0.250100</td>\n",
       "      <td>0.234123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 460 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Song Name      Genre       Tempo  Beats_Mean    Beats_SD  \\\n",
       "0         293    country.00093    country   86.132812  619.595238  365.842996   \n",
       "1         978       rock.00078       rock   99.384014  593.888889  342.603186   \n",
       "2         132  classical.00032  classical  103.359375  634.437500  346.060622   \n",
       "3         887     reggae.00087     reggae   83.354335  618.230769  349.762004   \n",
       "4         567       jazz.00067       jazz   99.384014  632.122449  363.047342   \n",
       "\n",
       "      Beats_1  Beats_1_SD     Beats_2  Beats_2_SD  ...  STFT_12_1  \\\n",
       "0  121.111111   73.283990  394.111111   77.730938  ...   0.149727   \n",
       "1  119.000000   67.993464  356.444444   67.906981  ...   0.498799   \n",
       "2  157.100000   71.568778  412.000000   75.288777  ...   0.140701   \n",
       "3  136.250000   71.140266  385.500000   71.029923  ...   0.515735   \n",
       "4  131.700000   74.037896  387.900000   73.305457  ...   0.111597   \n",
       "\n",
       "   STFT_12_1_SD  STFT_12_2  STFT_12_2_SD  STFT_12_3  STFT_12_3_SD  STFT_12_4  \\\n",
       "0      0.209647   0.257258      0.361308   0.206903      0.299516   0.203994   \n",
       "1      0.322212   0.299644      0.252775   0.228645      0.208432   0.335407   \n",
       "2      0.189802   0.135167      0.139561   0.074600      0.098088   0.153734   \n",
       "3      0.305537   0.537594      0.351125   0.464829      0.292321   0.407135   \n",
       "4      0.188841   0.345629      0.372253   0.316785      0.322037   0.183546   \n",
       "\n",
       "   STFT_12_4_SD  STFT_12_5  STFT_12_5_SD  \n",
       "0      0.301832   0.205054      0.270712  \n",
       "1      0.253260   0.277329      0.229559  \n",
       "2      0.173352   0.176123      0.133869  \n",
       "3      0.322346   0.430699      0.315258  \n",
       "4      0.258258   0.250100      0.234123  \n",
       "\n",
       "[5 rows x 460 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'Song Name', 'Genre', 'Tempo', 'Beats_Mean',\n",
       "       'Beats_SD', 'Beats_1', 'Beats_1_SD', 'Beats_2', 'Beats_2_SD',\n",
       "       'Beats_3', 'Beats_3_SD', 'Beats_4', 'Beats_4_SD', 'Beats_5',\n",
       "       'Beats_5_SD', 'RMSE', 'RMSE_SD', 'RMSE_1', 'RMSE_1_SD', 'RMSE_2',\n",
       "       'RMSE_2_SD', 'RMSE_3', 'RMSE_3_SD', 'RMSE_4', 'RMSE_4_SD',\n",
       "       'RMSE_5', 'RMSE_5_SD', 'SPEC_CENT', 'SPEC_CENT_SD', 'SPEC_CENT_1',\n",
       "       'SPEC_CENT_1_SD', 'SPEC_CENT_2', 'SPEC_CENT_2_SD', 'SPEC_CENT_3',\n",
       "       'SPEC_CENT_3_SD', 'SPEC_CENT_4', 'SPEC_CENT_4_SD', 'SPEC_CENT_5',\n",
       "       'SPEC_CENT_5_SD', 'SPEC_BW', 'SPEC_BW_SD', 'SPEC_BW_1',\n",
       "       'SPEC_BW_1_SD', 'SPEC_BW_2', 'SPEC_BW_2_SD', 'SPEC_BW_3',\n",
       "       'SPEC_BW_3_SD', 'SPEC_BW_4', 'SPEC_BW_4_SD', 'SPEC_BW_5',\n",
       "       'SPEC_BW_5_SD', 'ROLLOFF', 'ROLLOFF_SD', 'ROLLOFF_1',\n",
       "       'ROLLOFF_1_SD', 'ROLLOFF_2', 'ROLLOFF_2_SD', 'ROLLOFF_3',\n",
       "       'ROLLOFF_3_SD', 'ROLLOFF_4', 'ROLLOFF_4_SD', 'ROLLOFF_5',\n",
       "       'ROLLOFF_5_SD', 'ZCR', 'ZCR_SD', 'ZCR_1', 'ZCR_1_SD', 'ZCR_2',\n",
       "       'ZCR_2_SD', 'ZCR_3', 'ZCR_3_SD', 'ZCR_4', 'ZCR_4_SD', 'ZCR_5',\n",
       "       'ZCR_5_SD', 'MFCC_1', 'MFCC_1_SD', 'MFCC_1_1', 'MFCC_1_1_SD',\n",
       "       'MFCC_1_2', 'MFCC_1_2_SD', 'MFCC_1_3', 'MFCC_1_3_SD', 'MFCC_1_4',\n",
       "       'MFCC_1_4_SD', 'MFCC_1_5', 'MFCC_1_5_SD', 'MFCC_2', 'MFCC_2_SD',\n",
       "       'MFCC_2_1', 'MFCC_2_1_SD', 'MFCC_2_2', 'MFCC_2_2_SD', 'MFCC_2_3',\n",
       "       'MFCC_2_3_SD', 'MFCC_2_4', 'MFCC_2_4_SD', 'MFCC_2_5',\n",
       "       'MFCC_2_5_SD', 'MFCC_3', 'MFCC_3_SD', 'MFCC_3_1', 'MFCC_3_1_SD',\n",
       "       'MFCC_3_2', 'MFCC_3_2_SD', 'MFCC_3_3', 'MFCC_3_3_SD', 'MFCC_3_4',\n",
       "       'MFCC_3_4_SD', 'MFCC_3_5', 'MFCC_3_5_SD', 'MFCC_4', 'MFCC_4_SD',\n",
       "       'MFCC_4_1', 'MFCC_4_1_SD', 'MFCC_4_2', 'MFCC_4_2_SD', 'MFCC_4_3',\n",
       "       'MFCC_4_3_SD', 'MFCC_4_4', 'MFCC_4_4_SD', 'MFCC_4_5',\n",
       "       'MFCC_4_5_SD', 'MFCC_5', 'MFCC_5_SD', 'MFCC_5_1', 'MFCC_5_1_SD',\n",
       "       'MFCC_5_2', 'MFCC_5_2_SD', 'MFCC_5_3', 'MFCC_5_3_SD', 'MFCC_5_4',\n",
       "       'MFCC_5_4_SD', 'MFCC_5_5', 'MFCC_5_5_SD', 'MFCC_6', 'MFCC_6_SD',\n",
       "       'MFCC_6_1', 'MFCC_6_1_SD', 'MFCC_6_2', 'MFCC_6_2_SD', 'MFCC_6_3',\n",
       "       'MFCC_6_3_SD', 'MFCC_6_4', 'MFCC_6_4_SD', 'MFCC_6_5',\n",
       "       'MFCC_6_5_SD', 'MFCC_7', 'MFCC_7_SD', 'MFCC_7_1', 'MFCC_7_1_SD',\n",
       "       'MFCC_7_2', 'MFCC_7_2_SD', 'MFCC_7_3', 'MFCC_7_3_SD', 'MFCC_7_4',\n",
       "       'MFCC_7_4_SD', 'MFCC_7_5', 'MFCC_7_5_SD', 'MFCC_8', 'MFCC_8_SD',\n",
       "       'MFCC_8_1', 'MFCC_8_1_SD', 'MFCC_8_2', 'MFCC_8_2_SD', 'MFCC_8_3',\n",
       "       'MFCC_8_3_SD', 'MFCC_8_4', 'MFCC_8_4_SD', 'MFCC_8_5',\n",
       "       'MFCC_8_5_SD', 'MFCC_9', 'MFCC_9_SD', 'MFCC_9_1', 'MFCC_9_1_SD',\n",
       "       'MFCC_9_2', 'MFCC_9_2_SD', 'MFCC_9_3', 'MFCC_9_3_SD', 'MFCC_9_4',\n",
       "       'MFCC_9_4_SD', 'MFCC_9_5', 'MFCC_9_5_SD', 'MFCC_10', 'MFCC_10_SD',\n",
       "       'MFCC_10_1', 'MFCC_10_1_SD', 'MFCC_10_2', 'MFCC_10_2_SD',\n",
       "       'MFCC_10_3', 'MFCC_10_3_SD', 'MFCC_10_4', 'MFCC_10_4_SD',\n",
       "       'MFCC_10_5', 'MFCC_10_5_SD', 'MFCC_11', 'MFCC_11_SD', 'MFCC_11_1',\n",
       "       'MFCC_11_1_SD', 'MFCC_11_2', 'MFCC_11_2_SD', 'MFCC_11_3',\n",
       "       'MFCC_11_3_SD', 'MFCC_11_4', 'MFCC_11_4_SD', 'MFCC_11_5',\n",
       "       'MFCC_11_5_SD', 'MFCC_12', 'MFCC_12_SD', 'MFCC_12_1',\n",
       "       'MFCC_12_1_SD', 'MFCC_12_2', 'MFCC_12_2_SD', 'MFCC_12_3',\n",
       "       'MFCC_12_3_SD', 'MFCC_12_4', 'MFCC_12_4_SD', 'MFCC_12_5',\n",
       "       'MFCC_12_5_SD', 'MFCC_13', 'MFCC_13_SD', 'MFCC_13_1',\n",
       "       'MFCC_13_1_SD', 'MFCC_13_2', 'MFCC_13_2_SD', 'MFCC_13_3',\n",
       "       'MFCC_13_3_SD', 'MFCC_13_4', 'MFCC_13_4_SD', 'MFCC_13_5',\n",
       "       'MFCC_13_5_SD', 'MFCC_14', 'MFCC_14_SD', 'MFCC_14_1',\n",
       "       'MFCC_14_1_SD', 'MFCC_14_2', 'MFCC_14_2_SD', 'MFCC_14_3',\n",
       "       'MFCC_14_3_SD', 'MFCC_14_4', 'MFCC_14_4_SD', 'MFCC_14_5',\n",
       "       'MFCC_14_5_SD', 'MFCC_15', 'MFCC_15_SD', 'MFCC_15_1',\n",
       "       'MFCC_15_1_SD', 'MFCC_15_2', 'MFCC_15_2_SD', 'MFCC_15_3',\n",
       "       'MFCC_15_3_SD', 'MFCC_15_4', 'MFCC_15_4_SD', 'MFCC_15_5',\n",
       "       'MFCC_15_5_SD', 'MFCC_16', 'MFCC_16_SD', 'MFCC_16_1',\n",
       "       'MFCC_16_1_SD', 'MFCC_16_2', 'MFCC_16_2_SD', 'MFCC_16_3',\n",
       "       'MFCC_16_3_SD', 'MFCC_16_4', 'MFCC_16_4_SD', 'MFCC_16_5',\n",
       "       'MFCC_16_5_SD', 'MFCC_17', 'MFCC_17_SD', 'MFCC_17_1',\n",
       "       'MFCC_17_1_SD', 'MFCC_17_2', 'MFCC_17_2_SD', 'MFCC_17_3',\n",
       "       'MFCC_17_3_SD', 'MFCC_17_4', 'MFCC_17_4_SD', 'MFCC_17_5',\n",
       "       'MFCC_17_5_SD', 'MFCC_18', 'MFCC_18_SD', 'MFCC_18_1',\n",
       "       'MFCC_18_1_SD', 'MFCC_18_2', 'MFCC_18_2_SD', 'MFCC_18_3',\n",
       "       'MFCC_18_3_SD', 'MFCC_18_4', 'MFCC_18_4_SD', 'MFCC_18_5',\n",
       "       'MFCC_18_5_SD', 'MFCC_19', 'MFCC_19_SD', 'MFCC_19_1',\n",
       "       'MFCC_19_1_SD', 'MFCC_19_2', 'MFCC_19_2_SD', 'MFCC_19_3',\n",
       "       'MFCC_19_3_SD', 'MFCC_19_4', 'MFCC_19_4_SD', 'MFCC_19_5',\n",
       "       'MFCC_19_5_SD', 'MFCC_20', 'MFCC_20_SD', 'MFCC_20_1',\n",
       "       'MFCC_20_1_SD', 'MFCC_20_2', 'MFCC_20_2_SD', 'MFCC_20_3',\n",
       "       'MFCC_20_3_SD', 'MFCC_20_4', 'MFCC_20_4_SD', 'MFCC_20_5',\n",
       "       'MFCC_20_5_SD', 'STFT_1', 'STFT_1_SD', 'STFT_1_1', 'STFT_1_1_SD',\n",
       "       'STFT_1_2', 'STFT_1_2_SD', 'STFT_1_3', 'STFT_1_3_SD', 'STFT_1_4',\n",
       "       'STFT_1_4_SD', 'STFT_1_5', 'STFT_1_5_SD', 'STFT_2', 'STFT_2_SD',\n",
       "       'STFT_2_1', 'STFT_2_1_SD', 'STFT_2_2', 'STFT_2_2_SD', 'STFT_2_3',\n",
       "       'STFT_2_3_SD', 'STFT_2_4', 'STFT_2_4_SD', 'STFT_2_5',\n",
       "       'STFT_2_5_SD', 'STFT_3', 'STFT_3_SD', 'STFT_3_1', 'STFT_3_1_SD',\n",
       "       'STFT_3_2', 'STFT_3_2_SD', 'STFT_3_3', 'STFT_3_3_SD', 'STFT_3_4',\n",
       "       'STFT_3_4_SD', 'STFT_3_5', 'STFT_3_5_SD', 'STFT_4', 'STFT_4_SD',\n",
       "       'STFT_4_1', 'STFT_4_1_SD', 'STFT_4_2', 'STFT_4_2_SD', 'STFT_4_3',\n",
       "       'STFT_4_3_SD', 'STFT_4_4', 'STFT_4_4_SD', 'STFT_4_5',\n",
       "       'STFT_4_5_SD', 'STFT_5', 'STFT_5_SD', 'STFT_5_1', 'STFT_5_1_SD',\n",
       "       'STFT_5_2', 'STFT_5_2_SD', 'STFT_5_3', 'STFT_5_3_SD', 'STFT_5_4',\n",
       "       'STFT_5_4_SD', 'STFT_5_5', 'STFT_5_5_SD', 'STFT_6', 'STFT_6_SD',\n",
       "       'STFT_6_1', 'STFT_6_1_SD', 'STFT_6_2', 'STFT_6_2_SD', 'STFT_6_3',\n",
       "       'STFT_6_3_SD', 'STFT_6_4', 'STFT_6_4_SD', 'STFT_6_5',\n",
       "       'STFT_6_5_SD', 'STFT_7', 'STFT_7_SD', 'STFT_7_1', 'STFT_7_1_SD',\n",
       "       'STFT_7_2', 'STFT_7_2_SD', 'STFT_7_3', 'STFT_7_3_SD', 'STFT_7_4',\n",
       "       'STFT_7_4_SD', 'STFT_7_5', 'STFT_7_5_SD', 'STFT_8', 'STFT_8_SD',\n",
       "       'STFT_8_1', 'STFT_8_1_SD', 'STFT_8_2', 'STFT_8_2_SD', 'STFT_8_3',\n",
       "       'STFT_8_3_SD', 'STFT_8_4', 'STFT_8_4_SD', 'STFT_8_5',\n",
       "       'STFT_8_5_SD', 'STFT_9', 'STFT_9_SD', 'STFT_9_1', 'STFT_9_1_SD',\n",
       "       'STFT_9_2', 'STFT_9_2_SD', 'STFT_9_3', 'STFT_9_3_SD', 'STFT_9_4',\n",
       "       'STFT_9_4_SD', 'STFT_9_5', 'STFT_9_5_SD', 'STFT_10', 'STFT_10_SD',\n",
       "       'STFT_10_1', 'STFT_10_1_SD', 'STFT_10_2', 'STFT_10_2_SD',\n",
       "       'STFT_10_3', 'STFT_10_3_SD', 'STFT_10_4', 'STFT_10_4_SD',\n",
       "       'STFT_10_5', 'STFT_10_5_SD', 'STFT_11', 'STFT_11_SD', 'STFT_11_1',\n",
       "       'STFT_11_1_SD', 'STFT_11_2', 'STFT_11_2_SD', 'STFT_11_3',\n",
       "       'STFT_11_3_SD', 'STFT_11_4', 'STFT_11_4_SD', 'STFT_11_5',\n",
       "       'STFT_11_5_SD', 'STFT_12', 'STFT_12_SD', 'STFT_12_1',\n",
       "       'STFT_12_1_SD', 'STFT_12_2', 'STFT_12_2_SD', 'STFT_12_3',\n",
       "       'STFT_12_3_SD', 'STFT_12_4', 'STFT_12_4_SD', 'STFT_12_5',\n",
       "       'STFT_12_5_SD'], dtype=object)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['Song Name', 'Genre', 'Unnamed: 0'], axis = 1)\n",
    "y = df['Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "x = scaler.fit_transform(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[3, 6], 'n_estimators':[40, 50, 60, 70, 80, 90, 100], 'gamma':[0.01, 0.03, 0.05, 0.08, 0.1, 0.2],\n",
    "              'learning_rate':[0.05, 0.07, 0.1, 0.125, 0.15, 0.2]}\n",
    "\n",
    "rsc = RandomizedSearchCV(estimator = model, param_distributions = parameters, cv = 3, n_iter = 50, scoring = 'neg_log_loss', verbose = 0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26min 57s\n",
      "{'n_estimators': 90, 'max_depth': 3, 'learning_rate': 0.15, 'gamma': 0.05}\n"
     ]
    }
   ],
   "source": [
    "%time grid_result = rsc.fit(x_train, y_train)\n",
    "best_params = grid_result.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 40.5 s\n",
      "0.688\n",
      "Wall time: 46.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.fit(x_train, y_train)\n",
    "%time print(model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted confusion Matrix:\n",
      "[[15  0  2  0  0  2  2  0  0  0]\n",
      " [ 0 20  0  0  0  5  0  0  0  0]\n",
      " [ 1  0 21  1  0  0  1  2  0  1]\n",
      " [ 1  0  1 15  5  0  0  1  0  3]\n",
      " [ 2  0  1  0 12  0  1  1  5  1]\n",
      " [ 0  2  3  1  0 14  0  0  0  1]\n",
      " [ 0  0  0  0  0  0 26  0  0  2]\n",
      " [ 0  0  2  3  0  1  0 21  1  1]\n",
      " [ 0  0  1  0  2  0  0  2 15  1]\n",
      " [ 2  1  5  5  0  0  1  0  2 13]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71        21\n",
      "           1       0.87      0.80      0.83        25\n",
      "           2       0.58      0.78      0.67        27\n",
      "           3       0.60      0.58      0.59        26\n",
      "           4       0.63      0.52      0.57        23\n",
      "           5       0.64      0.67      0.65        21\n",
      "           6       0.84      0.93      0.88        28\n",
      "           7       0.78      0.72      0.75        29\n",
      "           8       0.65      0.71      0.68        21\n",
      "           9       0.57      0.45      0.50        29\n",
      "\n",
      "    accuracy                           0.69       250\n",
      "   macro avg       0.69      0.69      0.68       250\n",
      "weighted avg       0.69      0.69      0.68       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict(x_test)\n",
    "cm=confusion_matrix(y_test, predictions)\n",
    "print('Predicted confusion Matrix:')\n",
    "print(cm)\n",
    "print('')\n",
    "print('Classification Report: ')\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.LinearSVC(C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_svm = {'C':[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.15]}\n",
    "rsc_svm = RandomizedSearchCV(estimator = clf, param_distributions = parameters_svm, cv = 3, n_iter = 50, verbose = 0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 8 is smaller than n_iter=50. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.5 s\n",
      "{'C': 0.001}\n"
     ]
    }
   ],
   "source": [
    "%time grid_result_svm = rsc_svm.fit(x_train, y_train)\n",
    "best_params_svm = grid_result_svm.best_params_\n",
    "print(best_params_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.92 s\n",
      "0.576\n",
      "Wall time: 2 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%time clf.fit(x_train, y_train)\n",
    "%time print(clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted confusion Matrix:\n",
      "[[10  1  1  0  0  0  3  0  3  3]\n",
      " [ 2 17  0  1  0  1  0  0  2  2]\n",
      " [ 1  1 13  3  1  0  1  1  2  4]\n",
      " [ 0  0  1 10  6  1  1  6  1  0]\n",
      " [ 1  0  1  2 11  1  1  0  6  0]\n",
      " [ 0  1  1  0  0 15  0  0  2  2]\n",
      " [ 1  0  0  0  2  0 22  0  0  3]\n",
      " [ 2  0  3  0  3  0  0 20  1  0]\n",
      " [ 1  0  1  2  2  1  0  0 14  0]\n",
      " [ 0  1  3  6  2  2  0  2  1 12]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.48      0.51        21\n",
      "           1       0.81      0.68      0.74        25\n",
      "           2       0.54      0.48      0.51        27\n",
      "           3       0.42      0.38      0.40        26\n",
      "           4       0.41      0.48      0.44        23\n",
      "           5       0.71      0.71      0.71        21\n",
      "           6       0.79      0.79      0.79        28\n",
      "           7       0.69      0.69      0.69        29\n",
      "           8       0.44      0.67      0.53        21\n",
      "           9       0.46      0.41      0.44        29\n",
      "\n",
      "    accuracy                           0.58       250\n",
      "   macro avg       0.58      0.58      0.58       250\n",
      "weighted avg       0.59      0.58      0.58       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=clf.predict(x_test)\n",
    "cm=confusion_matrix(y_test, predictions)\n",
    "print('Predicted confusion Matrix:')\n",
    "print(cm)\n",
    "print('')\n",
    "print('Classification Report: ')\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, BatchNormalization, Activation, Dropout \n",
    "from keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(457,))\n",
    "\n",
    "x = Dense(64)(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dense(64)(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dense(64)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dense(16)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "predictions = Dense(10, activation = 'softmax')(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = predictions)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer = opt, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/500\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2673 - accuracy: 0.1800 - val_loss: 4.3983 - val_accuracy: 0.2600\n",
      "Epoch 2/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 1.8993 - accuracy: 0.3183 - val_loss: 5.4783 - val_accuracy: 0.2667\n",
      "Epoch 3/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 1.7157 - accuracy: 0.3600 - val_loss: 3.2475 - val_accuracy: 0.3133\n",
      "Epoch 4/500\n",
      "600/600 [==============================] - 0s 220us/step - loss: 1.5743 - accuracy: 0.4100 - val_loss: 2.5344 - val_accuracy: 0.3867\n",
      "Epoch 5/500\n",
      "600/600 [==============================] - 0s 218us/step - loss: 1.5391 - accuracy: 0.4317 - val_loss: 1.9900 - val_accuracy: 0.3800\n",
      "Epoch 6/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 1.3925 - accuracy: 0.4933 - val_loss: 1.9894 - val_accuracy: 0.4333\n",
      "Epoch 7/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 1.3643 - accuracy: 0.4717 - val_loss: 1.6338 - val_accuracy: 0.4800\n",
      "Epoch 8/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 1.2182 - accuracy: 0.5517 - val_loss: 1.7865 - val_accuracy: 0.4533\n",
      "Epoch 9/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 1.1913 - accuracy: 0.5583 - val_loss: 1.5619 - val_accuracy: 0.5467\n",
      "Epoch 10/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 1.1413 - accuracy: 0.5783 - val_loss: 1.5735 - val_accuracy: 0.5800\n",
      "Epoch 11/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 1.1279 - accuracy: 0.6133 - val_loss: 1.5786 - val_accuracy: 0.5667\n",
      "Epoch 12/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 1.0345 - accuracy: 0.6550 - val_loss: 1.6007 - val_accuracy: 0.5467\n",
      "Epoch 13/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 1.0163 - accuracy: 0.6533 - val_loss: 1.5075 - val_accuracy: 0.5667\n",
      "Epoch 14/500\n",
      "600/600 [==============================] - 0s 259us/step - loss: 0.9860 - accuracy: 0.6717 - val_loss: 1.5039 - val_accuracy: 0.6000\n",
      "Epoch 15/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.8798 - accuracy: 0.7100 - val_loss: 1.5175 - val_accuracy: 0.5933\n",
      "Epoch 16/500\n",
      "600/600 [==============================] - 0s 249us/step - loss: 0.9126 - accuracy: 0.6917 - val_loss: 1.4208 - val_accuracy: 0.6200\n",
      "Epoch 17/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.8852 - accuracy: 0.7150 - val_loss: 1.3479 - val_accuracy: 0.6400\n",
      "Epoch 18/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.8093 - accuracy: 0.7367 - val_loss: 1.3992 - val_accuracy: 0.6267\n",
      "Epoch 19/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.8737 - accuracy: 0.7133 - val_loss: 1.4785 - val_accuracy: 0.6067\n",
      "Epoch 20/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.7306 - accuracy: 0.7583 - val_loss: 1.3823 - val_accuracy: 0.6467\n",
      "Epoch 21/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.7706 - accuracy: 0.7500 - val_loss: 1.2859 - val_accuracy: 0.6667\n",
      "Epoch 22/500\n",
      "600/600 [==============================] - 0s 259us/step - loss: 0.7595 - accuracy: 0.7317 - val_loss: 1.2421 - val_accuracy: 0.6600\n",
      "Epoch 23/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.7028 - accuracy: 0.7850 - val_loss: 1.2659 - val_accuracy: 0.6400\n",
      "Epoch 24/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.7938 - accuracy: 0.7533 - val_loss: 1.5868 - val_accuracy: 0.6333\n",
      "Epoch 25/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.7213 - accuracy: 0.7733 - val_loss: 1.2910 - val_accuracy: 0.6667\n",
      "Epoch 26/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.6432 - accuracy: 0.8033 - val_loss: 1.1962 - val_accuracy: 0.6933\n",
      "Epoch 27/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.6367 - accuracy: 0.7867 - val_loss: 1.2350 - val_accuracy: 0.6933\n",
      "Epoch 28/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.6331 - accuracy: 0.8017 - val_loss: 1.2489 - val_accuracy: 0.6467\n",
      "Epoch 29/500\n",
      "600/600 [==============================] - 0s 255us/step - loss: 0.6737 - accuracy: 0.7767 - val_loss: 1.3216 - val_accuracy: 0.6533\n",
      "Epoch 30/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.5865 - accuracy: 0.8283 - val_loss: 1.3581 - val_accuracy: 0.6533\n",
      "Epoch 31/500\n",
      "600/600 [==============================] - 0s 255us/step - loss: 0.5486 - accuracy: 0.8233 - val_loss: 1.2936 - val_accuracy: 0.6933\n",
      "Epoch 32/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.5708 - accuracy: 0.8150 - val_loss: 1.3666 - val_accuracy: 0.6800\n",
      "Epoch 33/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.5328 - accuracy: 0.8250 - val_loss: 1.2687 - val_accuracy: 0.6800\n",
      "Epoch 34/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.5103 - accuracy: 0.8483 - val_loss: 1.3640 - val_accuracy: 0.6733\n",
      "Epoch 35/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.5987 - accuracy: 0.8167 - val_loss: 1.3413 - val_accuracy: 0.6867\n",
      "Epoch 36/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.4857 - accuracy: 0.8367 - val_loss: 1.1549 - val_accuracy: 0.7067\n",
      "Epoch 37/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.5124 - accuracy: 0.8633 - val_loss: 1.3280 - val_accuracy: 0.7000\n",
      "Epoch 38/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.5202 - accuracy: 0.8433 - val_loss: 1.3207 - val_accuracy: 0.6867\n",
      "Epoch 39/500\n",
      "600/600 [==============================] - 0s 259us/step - loss: 0.5214 - accuracy: 0.8500 - val_loss: 1.3376 - val_accuracy: 0.7000\n",
      "Epoch 40/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.5007 - accuracy: 0.8517 - val_loss: 1.2179 - val_accuracy: 0.6933\n",
      "Epoch 41/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.4530 - accuracy: 0.8800 - val_loss: 1.1845 - val_accuracy: 0.7133\n",
      "Epoch 42/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.4243 - accuracy: 0.8750 - val_loss: 1.4125 - val_accuracy: 0.6800\n",
      "Epoch 43/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.5025 - accuracy: 0.8550 - val_loss: 1.3194 - val_accuracy: 0.6667\n",
      "Epoch 44/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.5514 - accuracy: 0.8383 - val_loss: 1.4285 - val_accuracy: 0.6667\n",
      "Epoch 45/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.4975 - accuracy: 0.8683 - val_loss: 1.3633 - val_accuracy: 0.6733\n",
      "Epoch 46/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.4218 - accuracy: 0.8667 - val_loss: 1.3029 - val_accuracy: 0.6733\n",
      "Epoch 47/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.4063 - accuracy: 0.8833 - val_loss: 1.3913 - val_accuracy: 0.6400\n",
      "Epoch 48/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.4979 - accuracy: 0.8400 - val_loss: 1.5820 - val_accuracy: 0.6333\n",
      "Epoch 49/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.4248 - accuracy: 0.8700 - val_loss: 1.4537 - val_accuracy: 0.6467\n",
      "Epoch 50/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.4631 - accuracy: 0.8600 - val_loss: 1.5724 - val_accuracy: 0.6867\n",
      "Epoch 51/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.4916 - accuracy: 0.8400 - val_loss: 1.6388 - val_accuracy: 0.6400\n",
      "Epoch 52/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.3666 - accuracy: 0.9083 - val_loss: 1.3924 - val_accuracy: 0.6800\n",
      "Epoch 53/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.4283 - accuracy: 0.8650 - val_loss: 1.4337 - val_accuracy: 0.6733\n",
      "Epoch 54/500\n",
      "600/600 [==============================] - 0s 250us/step - loss: 0.3330 - accuracy: 0.9117 - val_loss: 1.3917 - val_accuracy: 0.6867\n",
      "Epoch 55/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.4263 - accuracy: 0.8850 - val_loss: 1.4417 - val_accuracy: 0.6533\n",
      "Epoch 56/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.4141 - accuracy: 0.8900 - val_loss: 1.4188 - val_accuracy: 0.6600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.3404 - accuracy: 0.8833 - val_loss: 1.5144 - val_accuracy: 0.6733\n",
      "Epoch 58/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.3694 - accuracy: 0.8800 - val_loss: 1.4513 - val_accuracy: 0.6600\n",
      "Epoch 59/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.4501 - accuracy: 0.8783 - val_loss: 1.5556 - val_accuracy: 0.6467\n",
      "Epoch 60/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.3620 - accuracy: 0.8933 - val_loss: 1.4333 - val_accuracy: 0.6600\n",
      "Epoch 61/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.3351 - accuracy: 0.9117 - val_loss: 1.4517 - val_accuracy: 0.6600\n",
      "Epoch 62/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.3324 - accuracy: 0.9017 - val_loss: 1.4364 - val_accuracy: 0.6933\n",
      "Epoch 63/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.2845 - accuracy: 0.9083 - val_loss: 1.3522 - val_accuracy: 0.7000\n",
      "Epoch 64/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.2864 - accuracy: 0.9167 - val_loss: 1.3220 - val_accuracy: 0.7133\n",
      "Epoch 65/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.2661 - accuracy: 0.9217 - val_loss: 1.4146 - val_accuracy: 0.7067\n",
      "Epoch 66/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.2462 - accuracy: 0.9350 - val_loss: 1.4806 - val_accuracy: 0.6933\n",
      "Epoch 67/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.3827 - accuracy: 0.8950 - val_loss: 1.5660 - val_accuracy: 0.6733\n",
      "Epoch 68/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.3478 - accuracy: 0.8950 - val_loss: 1.3434 - val_accuracy: 0.6800\n",
      "Epoch 69/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.3513 - accuracy: 0.8983 - val_loss: 1.5257 - val_accuracy: 0.6733\n",
      "Epoch 70/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.3206 - accuracy: 0.9233 - val_loss: 1.5787 - val_accuracy: 0.6867\n",
      "Epoch 71/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.3386 - accuracy: 0.9017 - val_loss: 1.5669 - val_accuracy: 0.6667\n",
      "Epoch 72/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.3715 - accuracy: 0.8950 - val_loss: 1.3806 - val_accuracy: 0.6867\n",
      "Epoch 73/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.3380 - accuracy: 0.9000 - val_loss: 1.4944 - val_accuracy: 0.6800\n",
      "Epoch 74/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.2659 - accuracy: 0.9117 - val_loss: 1.3220 - val_accuracy: 0.7133\n",
      "Epoch 75/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.2482 - accuracy: 0.9333 - val_loss: 1.3068 - val_accuracy: 0.7000\n",
      "Epoch 76/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.3062 - accuracy: 0.9067 - val_loss: 1.3716 - val_accuracy: 0.6933\n",
      "Epoch 77/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.3443 - accuracy: 0.9117 - val_loss: 1.4575 - val_accuracy: 0.6933\n",
      "Epoch 78/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.4466 - accuracy: 0.8700 - val_loss: 1.3733 - val_accuracy: 0.7000\n",
      "Epoch 79/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.3480 - accuracy: 0.8983 - val_loss: 1.4371 - val_accuracy: 0.6733\n",
      "Epoch 80/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.2966 - accuracy: 0.9150 - val_loss: 1.4226 - val_accuracy: 0.6733\n",
      "Epoch 81/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.3179 - accuracy: 0.9100 - val_loss: 1.3758 - val_accuracy: 0.6600\n",
      "Epoch 82/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.3477 - accuracy: 0.9017 - val_loss: 1.2600 - val_accuracy: 0.7333\n",
      "Epoch 83/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.3967 - accuracy: 0.8850 - val_loss: 1.2366 - val_accuracy: 0.7267\n",
      "Epoch 84/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.3646 - accuracy: 0.8783 - val_loss: 1.3056 - val_accuracy: 0.6800\n",
      "Epoch 85/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.2478 - accuracy: 0.9267 - val_loss: 1.3129 - val_accuracy: 0.6800\n",
      "Epoch 86/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.3253 - accuracy: 0.9117 - val_loss: 1.3700 - val_accuracy: 0.7000\n",
      "Epoch 87/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.2960 - accuracy: 0.9217 - val_loss: 1.4119 - val_accuracy: 0.7000\n",
      "Epoch 88/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.2920 - accuracy: 0.9217 - val_loss: 1.5337 - val_accuracy: 0.6667\n",
      "Epoch 89/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.3801 - accuracy: 0.8867 - val_loss: 1.4505 - val_accuracy: 0.6800\n",
      "Epoch 90/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.2957 - accuracy: 0.9117 - val_loss: 1.5150 - val_accuracy: 0.6867\n",
      "Epoch 91/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.2608 - accuracy: 0.9233 - val_loss: 1.4313 - val_accuracy: 0.7133\n",
      "Epoch 92/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.2826 - accuracy: 0.9200 - val_loss: 1.3492 - val_accuracy: 0.7333\n",
      "Epoch 93/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.2326 - accuracy: 0.9367 - val_loss: 1.4051 - val_accuracy: 0.7067\n",
      "Epoch 94/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.2637 - accuracy: 0.9417 - val_loss: 1.4126 - val_accuracy: 0.7067\n",
      "Epoch 95/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.2581 - accuracy: 0.9167 - val_loss: 1.3651 - val_accuracy: 0.7467\n",
      "Epoch 96/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.2294 - accuracy: 0.9367 - val_loss: 1.4758 - val_accuracy: 0.7000\n",
      "Epoch 97/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.2617 - accuracy: 0.9350 - val_loss: 1.4989 - val_accuracy: 0.6933\n",
      "Epoch 98/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.2256 - accuracy: 0.9400 - val_loss: 1.4754 - val_accuracy: 0.6800\n",
      "Epoch 99/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.2822 - accuracy: 0.9167 - val_loss: 1.4369 - val_accuracy: 0.7067\n",
      "Epoch 100/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.2205 - accuracy: 0.9450 - val_loss: 1.3467 - val_accuracy: 0.7267\n",
      "Epoch 101/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.2413 - accuracy: 0.9250 - val_loss: 1.3390 - val_accuracy: 0.7200\n",
      "Epoch 102/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.2518 - accuracy: 0.9183 - val_loss: 1.4093 - val_accuracy: 0.7067\n",
      "Epoch 103/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.2590 - accuracy: 0.9200 - val_loss: 1.4672 - val_accuracy: 0.7267\n",
      "Epoch 104/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1927 - accuracy: 0.9417 - val_loss: 1.4165 - val_accuracy: 0.7133\n",
      "Epoch 105/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.2119 - accuracy: 0.9350 - val_loss: 1.4881 - val_accuracy: 0.7067\n",
      "Epoch 106/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.2549 - accuracy: 0.9333 - val_loss: 1.5164 - val_accuracy: 0.6867\n",
      "Epoch 107/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.2959 - accuracy: 0.9200 - val_loss: 1.5695 - val_accuracy: 0.6533\n",
      "Epoch 108/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.3268 - accuracy: 0.9133 - val_loss: 1.6222 - val_accuracy: 0.6533\n",
      "Epoch 109/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.2816 - accuracy: 0.9200 - val_loss: 1.4498 - val_accuracy: 0.6800\n",
      "Epoch 110/500\n",
      "600/600 [==============================] - 0s 260us/step - loss: 0.2351 - accuracy: 0.9367 - val_loss: 1.5076 - val_accuracy: 0.6867\n",
      "Epoch 111/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.2739 - accuracy: 0.9333 - val_loss: 1.6042 - val_accuracy: 0.7000\n",
      "Epoch 112/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.2865 - accuracy: 0.9417 - val_loss: 1.5383 - val_accuracy: 0.6800\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 233us/step - loss: 0.2210 - accuracy: 0.9383 - val_loss: 1.5748 - val_accuracy: 0.6600\n",
      "Epoch 114/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.3134 - accuracy: 0.9133 - val_loss: 1.4182 - val_accuracy: 0.7200\n",
      "Epoch 115/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.2208 - accuracy: 0.9400 - val_loss: 1.4466 - val_accuracy: 0.7067\n",
      "Epoch 116/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.2311 - accuracy: 0.9383 - val_loss: 1.5431 - val_accuracy: 0.6933\n",
      "Epoch 117/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.2342 - accuracy: 0.9317 - val_loss: 1.5345 - val_accuracy: 0.7000\n",
      "Epoch 118/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.1760 - accuracy: 0.9517 - val_loss: 1.5834 - val_accuracy: 0.6800\n",
      "Epoch 119/500\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.2055 - accuracy: 0.9417 - val_loss: 1.5265 - val_accuracy: 0.7000\n",
      "Epoch 120/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.1924 - accuracy: 0.9483 - val_loss: 1.5475 - val_accuracy: 0.6533\n",
      "Epoch 121/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.2181 - accuracy: 0.9350 - val_loss: 1.3920 - val_accuracy: 0.6800\n",
      "Epoch 122/500\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.2601 - accuracy: 0.9267 - val_loss: 1.4551 - val_accuracy: 0.7133\n",
      "Epoch 123/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.2380 - accuracy: 0.9333 - val_loss: 1.4096 - val_accuracy: 0.7067\n",
      "Epoch 124/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.2077 - accuracy: 0.9450 - val_loss: 1.4563 - val_accuracy: 0.6933\n",
      "Epoch 125/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.2138 - accuracy: 0.9467 - val_loss: 1.3967 - val_accuracy: 0.7133\n",
      "Epoch 126/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.2439 - accuracy: 0.9333 - val_loss: 1.3683 - val_accuracy: 0.7133\n",
      "Epoch 127/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.2876 - accuracy: 0.9200 - val_loss: 1.5651 - val_accuracy: 0.6733\n",
      "Epoch 128/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.2546 - accuracy: 0.9267 - val_loss: 1.5300 - val_accuracy: 0.6867\n",
      "Epoch 129/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1883 - accuracy: 0.9500 - val_loss: 1.5126 - val_accuracy: 0.6933\n",
      "Epoch 130/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1984 - accuracy: 0.9483 - val_loss: 1.4977 - val_accuracy: 0.6933\n",
      "Epoch 131/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.2383 - accuracy: 0.9350 - val_loss: 1.3914 - val_accuracy: 0.7000\n",
      "Epoch 132/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.2022 - accuracy: 0.9533 - val_loss: 1.4537 - val_accuracy: 0.7000\n",
      "Epoch 133/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.1888 - accuracy: 0.9517 - val_loss: 1.4283 - val_accuracy: 0.7000\n",
      "Epoch 134/500\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.1803 - accuracy: 0.94 - 0s 233us/step - loss: 0.1866 - accuracy: 0.9433 - val_loss: 1.4732 - val_accuracy: 0.6667\n",
      "Epoch 135/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.2096 - accuracy: 0.9500 - val_loss: 1.4276 - val_accuracy: 0.6800\n",
      "Epoch 136/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.2236 - accuracy: 0.9400 - val_loss: 1.5377 - val_accuracy: 0.6933\n",
      "Epoch 137/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1697 - accuracy: 0.9517 - val_loss: 1.4651 - val_accuracy: 0.7000\n",
      "Epoch 138/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.2053 - accuracy: 0.9367 - val_loss: 1.3514 - val_accuracy: 0.7333\n",
      "Epoch 139/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.1759 - accuracy: 0.9417 - val_loss: 1.4217 - val_accuracy: 0.7000\n",
      "Epoch 140/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1904 - accuracy: 0.9467 - val_loss: 1.4424 - val_accuracy: 0.6733\n",
      "Epoch 141/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.1959 - accuracy: 0.9533 - val_loss: 1.6779 - val_accuracy: 0.6800\n",
      "Epoch 142/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.2280 - accuracy: 0.9400 - val_loss: 1.5347 - val_accuracy: 0.7333\n",
      "Epoch 143/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.2631 - accuracy: 0.9183 - val_loss: 1.4841 - val_accuracy: 0.7067\n",
      "Epoch 144/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.2364 - accuracy: 0.9433 - val_loss: 1.5403 - val_accuracy: 0.7000\n",
      "Epoch 145/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.2087 - accuracy: 0.9383 - val_loss: 1.4859 - val_accuracy: 0.7267\n",
      "Epoch 146/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1701 - accuracy: 0.9450 - val_loss: 1.4762 - val_accuracy: 0.7200\n",
      "Epoch 147/500\n",
      "600/600 [==============================] - 0s 252us/step - loss: 0.1596 - accuracy: 0.9400 - val_loss: 1.5396 - val_accuracy: 0.7333\n",
      "Epoch 148/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.2183 - accuracy: 0.9417 - val_loss: 1.6544 - val_accuracy: 0.7000\n",
      "Epoch 149/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.2130 - accuracy: 0.9400 - val_loss: 1.4690 - val_accuracy: 0.7267\n",
      "Epoch 150/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1899 - accuracy: 0.9483 - val_loss: 1.4543 - val_accuracy: 0.7333\n",
      "Epoch 151/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1852 - accuracy: 0.9517 - val_loss: 1.4189 - val_accuracy: 0.7200\n",
      "Epoch 152/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.2220 - accuracy: 0.9317 - val_loss: 1.4695 - val_accuracy: 0.6867\n",
      "Epoch 153/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1838 - accuracy: 0.9533 - val_loss: 1.4244 - val_accuracy: 0.7133\n",
      "Epoch 154/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1720 - accuracy: 0.9483 - val_loss: 1.5215 - val_accuracy: 0.7067\n",
      "Epoch 155/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1970 - accuracy: 0.9517 - val_loss: 1.5366 - val_accuracy: 0.7400\n",
      "Epoch 156/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1471 - accuracy: 0.9600 - val_loss: 1.5059 - val_accuracy: 0.7267\n",
      "Epoch 157/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.2037 - accuracy: 0.9317 - val_loss: 1.4689 - val_accuracy: 0.7200\n",
      "Epoch 158/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.2665 - accuracy: 0.9350 - val_loss: 1.4479 - val_accuracy: 0.6800\n",
      "Epoch 159/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1746 - accuracy: 0.9450 - val_loss: 1.5387 - val_accuracy: 0.7000\n",
      "Epoch 160/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.1851 - accuracy: 0.9500 - val_loss: 1.4733 - val_accuracy: 0.7067\n",
      "Epoch 161/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1594 - accuracy: 0.9533 - val_loss: 1.5219 - val_accuracy: 0.6933\n",
      "Epoch 162/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.2744 - accuracy: 0.9333 - val_loss: 1.5928 - val_accuracy: 0.6733\n",
      "Epoch 163/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.2321 - accuracy: 0.9400 - val_loss: 1.5412 - val_accuracy: 0.6867\n",
      "Epoch 164/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1941 - accuracy: 0.9400 - val_loss: 1.5526 - val_accuracy: 0.7000\n",
      "Epoch 165/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.1849 - accuracy: 0.9500 - val_loss: 1.5966 - val_accuracy: 0.6733\n",
      "Epoch 166/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.1762 - accuracy: 0.9450 - val_loss: 1.5437 - val_accuracy: 0.6733\n",
      "Epoch 167/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.3021 - accuracy: 0.9200 - val_loss: 1.6845 - val_accuracy: 0.6600\n",
      "Epoch 168/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1395 - accuracy: 0.9583 - val_loss: 1.5574 - val_accuracy: 0.6667\n",
      "Epoch 169/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.1866 - accuracy: 0.9533 - val_loss: 1.4668 - val_accuracy: 0.7267\n",
      "Epoch 170/500\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.2020 - accuracy: 0.9500 - val_loss: 1.3844 - val_accuracy: 0.7067\n",
      "Epoch 171/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.1371 - accuracy: 0.9617 - val_loss: 1.4199 - val_accuracy: 0.7067\n",
      "Epoch 172/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.1463 - accuracy: 0.9567 - val_loss: 1.5409 - val_accuracy: 0.6600\n",
      "Epoch 173/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.1451 - accuracy: 0.9550 - val_loss: 1.3412 - val_accuracy: 0.6867\n",
      "Epoch 174/500\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.1436 - accuracy: 0.9600 - val_loss: 1.4705 - val_accuracy: 0.6800\n",
      "Epoch 175/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.1635 - accuracy: 0.9567 - val_loss: 1.5296 - val_accuracy: 0.7067\n",
      "Epoch 176/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.1757 - accuracy: 0.9550 - val_loss: 1.4159 - val_accuracy: 0.7067\n",
      "Epoch 177/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.1680 - accuracy: 0.9533 - val_loss: 1.6161 - val_accuracy: 0.6733\n",
      "Epoch 178/500\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.1563 - accuracy: 0.9500 - val_loss: 1.3888 - val_accuracy: 0.7133\n",
      "Epoch 179/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.1956 - accuracy: 0.9500 - val_loss: 1.4219 - val_accuracy: 0.7133\n",
      "Epoch 180/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.2630 - accuracy: 0.9383 - val_loss: 1.3813 - val_accuracy: 0.7200\n",
      "Epoch 181/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.1888 - accuracy: 0.9417 - val_loss: 1.2349 - val_accuracy: 0.7333\n",
      "Epoch 182/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.2784 - accuracy: 0.9233 - val_loss: 1.3359 - val_accuracy: 0.7133\n",
      "Epoch 183/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.2020 - accuracy: 0.9450 - val_loss: 1.4067 - val_accuracy: 0.6800\n",
      "Epoch 184/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1851 - accuracy: 0.9400 - val_loss: 1.4969 - val_accuracy: 0.6667\n",
      "Epoch 185/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.1613 - accuracy: 0.9550 - val_loss: 1.5982 - val_accuracy: 0.6867\n",
      "Epoch 186/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1171 - accuracy: 0.9667 - val_loss: 1.5698 - val_accuracy: 0.6867\n",
      "Epoch 187/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.1161 - accuracy: 0.9650 - val_loss: 1.6196 - val_accuracy: 0.6667\n",
      "Epoch 188/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1419 - accuracy: 0.9567 - val_loss: 1.6997 - val_accuracy: 0.6933\n",
      "Epoch 189/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.2135 - accuracy: 0.9500 - val_loss: 1.5070 - val_accuracy: 0.6933\n",
      "Epoch 190/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1629 - accuracy: 0.9533 - val_loss: 1.4982 - val_accuracy: 0.7133\n",
      "Epoch 191/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.2014 - accuracy: 0.9500 - val_loss: 1.4892 - val_accuracy: 0.6933\n",
      "Epoch 192/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.2374 - accuracy: 0.9367 - val_loss: 1.4654 - val_accuracy: 0.6867\n",
      "Epoch 193/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1404 - accuracy: 0.9667 - val_loss: 1.4491 - val_accuracy: 0.7000\n",
      "Epoch 194/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.1988 - accuracy: 0.9417 - val_loss: 1.4695 - val_accuracy: 0.6867\n",
      "Epoch 195/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.2007 - accuracy: 0.9500 - val_loss: 1.4570 - val_accuracy: 0.7333\n",
      "Epoch 196/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1768 - accuracy: 0.9550 - val_loss: 1.4322 - val_accuracy: 0.7200\n",
      "Epoch 197/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.1643 - accuracy: 0.9483 - val_loss: 1.5905 - val_accuracy: 0.6933\n",
      "Epoch 198/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.1685 - accuracy: 0.9550 - val_loss: 1.6277 - val_accuracy: 0.6867\n",
      "Epoch 199/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.2122 - accuracy: 0.9383 - val_loss: 1.7521 - val_accuracy: 0.6467\n",
      "Epoch 200/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1857 - accuracy: 0.9467 - val_loss: 1.6449 - val_accuracy: 0.6867\n",
      "Epoch 201/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1530 - accuracy: 0.9667 - val_loss: 1.6453 - val_accuracy: 0.6600\n",
      "Epoch 202/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.2358 - accuracy: 0.9333 - val_loss: 1.6010 - val_accuracy: 0.6800\n",
      "Epoch 203/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.1323 - accuracy: 0.9700 - val_loss: 1.6462 - val_accuracy: 0.6533\n",
      "Epoch 204/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1923 - accuracy: 0.9433 - val_loss: 1.5869 - val_accuracy: 0.6867\n",
      "Epoch 205/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1736 - accuracy: 0.9517 - val_loss: 1.4809 - val_accuracy: 0.7067\n",
      "Epoch 206/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1509 - accuracy: 0.9483 - val_loss: 1.5640 - val_accuracy: 0.7000\n",
      "Epoch 207/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1332 - accuracy: 0.9717 - val_loss: 1.5114 - val_accuracy: 0.7200\n",
      "Epoch 208/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.1163 - accuracy: 0.9683 - val_loss: 1.5288 - val_accuracy: 0.7133\n",
      "Epoch 209/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1557 - accuracy: 0.9600 - val_loss: 1.5825 - val_accuracy: 0.6867\n",
      "Epoch 210/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1244 - accuracy: 0.9633 - val_loss: 1.5163 - val_accuracy: 0.7000\n",
      "Epoch 211/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.1786 - accuracy: 0.9583 - val_loss: 1.4981 - val_accuracy: 0.6867\n",
      "Epoch 212/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1362 - accuracy: 0.9667 - val_loss: 1.4700 - val_accuracy: 0.7067\n",
      "Epoch 213/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1007 - accuracy: 0.9733 - val_loss: 1.4563 - val_accuracy: 0.7067\n",
      "Epoch 214/500\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.1943 - accuracy: 0.9583 - val_loss: 1.6186 - val_accuracy: 0.6600\n",
      "Epoch 215/500\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.1730 - accuracy: 0.9583 - val_loss: 1.5714 - val_accuracy: 0.6733\n",
      "Epoch 216/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.1516 - accuracy: 0.9567 - val_loss: 1.6125 - val_accuracy: 0.6667\n",
      "Epoch 217/500\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.0911 - accuracy: 0.9750 - val_loss: 1.5709 - val_accuracy: 0.6867\n",
      "Epoch 218/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1376 - accuracy: 0.9617 - val_loss: 1.4329 - val_accuracy: 0.7200\n",
      "Epoch 219/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1380 - accuracy: 0.9667 - val_loss: 1.4941 - val_accuracy: 0.7067\n",
      "Epoch 220/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.0934 - accuracy: 0.9717 - val_loss: 1.6055 - val_accuracy: 0.7000\n",
      "Epoch 221/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.1392 - accuracy: 0.9633 - val_loss: 1.6296 - val_accuracy: 0.6800\n",
      "Epoch 222/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1069 - accuracy: 0.9700 - val_loss: 1.5734 - val_accuracy: 0.7200\n",
      "Epoch 223/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1317 - accuracy: 0.9567 - val_loss: 1.6779 - val_accuracy: 0.7133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1522 - accuracy: 0.9567 - val_loss: 1.7702 - val_accuracy: 0.6733\n",
      "Epoch 225/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1508 - accuracy: 0.9533 - val_loss: 1.6604 - val_accuracy: 0.6933\n",
      "Epoch 226/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1927 - accuracy: 0.9450 - val_loss: 1.6777 - val_accuracy: 0.6867\n",
      "Epoch 227/500\n",
      "600/600 [==============================] - 0s 259us/step - loss: 0.1535 - accuracy: 0.9600 - val_loss: 1.6757 - val_accuracy: 0.7000\n",
      "Epoch 228/500\n",
      "600/600 [==============================] - 0s 260us/step - loss: 0.1286 - accuracy: 0.9500 - val_loss: 1.6893 - val_accuracy: 0.7200\n",
      "Epoch 229/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.1478 - accuracy: 0.9617 - val_loss: 1.7490 - val_accuracy: 0.6933\n",
      "Epoch 230/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1557 - accuracy: 0.9483 - val_loss: 1.6292 - val_accuracy: 0.7067\n",
      "Epoch 231/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.1452 - accuracy: 0.9533 - val_loss: 1.6243 - val_accuracy: 0.7067\n",
      "Epoch 232/500\n",
      "600/600 [==============================] - 0s 257us/step - loss: 0.1931 - accuracy: 0.9567 - val_loss: 1.6589 - val_accuracy: 0.7067\n",
      "Epoch 233/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.1038 - accuracy: 0.9717 - val_loss: 1.6691 - val_accuracy: 0.6933\n",
      "Epoch 234/500\n",
      "600/600 [==============================] - 0s 254us/step - loss: 0.1915 - accuracy: 0.9450 - val_loss: 1.6362 - val_accuracy: 0.7133\n",
      "Epoch 235/500\n",
      "600/600 [==============================] - 0s 265us/step - loss: 0.1348 - accuracy: 0.9550 - val_loss: 1.5721 - val_accuracy: 0.7133\n",
      "Epoch 236/500\n",
      "600/600 [==============================] - 0s 255us/step - loss: 0.1765 - accuracy: 0.9567 - val_loss: 1.5422 - val_accuracy: 0.7200\n",
      "Epoch 237/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1981 - accuracy: 0.9467 - val_loss: 1.6641 - val_accuracy: 0.6533\n",
      "Epoch 238/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.1479 - accuracy: 0.9633 - val_loss: 1.7584 - val_accuracy: 0.6667\n",
      "Epoch 239/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1148 - accuracy: 0.9733 - val_loss: 1.7558 - val_accuracy: 0.6667\n",
      "Epoch 240/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1340 - accuracy: 0.9683 - val_loss: 1.5886 - val_accuracy: 0.6933\n",
      "Epoch 241/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.1021 - accuracy: 0.9783 - val_loss: 1.5282 - val_accuracy: 0.7200\n",
      "Epoch 242/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1336 - accuracy: 0.9567 - val_loss: 1.5437 - val_accuracy: 0.7067\n",
      "Epoch 243/500\n",
      "600/600 [==============================] - 0s 252us/step - loss: 0.1328 - accuracy: 0.9683 - val_loss: 1.6845 - val_accuracy: 0.6867\n",
      "Epoch 244/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1374 - accuracy: 0.9683 - val_loss: 1.6004 - val_accuracy: 0.7000\n",
      "Epoch 245/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1168 - accuracy: 0.9617 - val_loss: 1.6497 - val_accuracy: 0.6733\n",
      "Epoch 246/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.0839 - accuracy: 0.9800 - val_loss: 1.6814 - val_accuracy: 0.6800\n",
      "Epoch 247/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.1575 - accuracy: 0.9533 - val_loss: 1.8140 - val_accuracy: 0.6867\n",
      "Epoch 248/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1078 - accuracy: 0.9717 - val_loss: 1.6721 - val_accuracy: 0.7000\n",
      "Epoch 249/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1197 - accuracy: 0.9683 - val_loss: 1.7710 - val_accuracy: 0.7067\n",
      "Epoch 250/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.1496 - accuracy: 0.9617 - val_loss: 1.6793 - val_accuracy: 0.7133\n",
      "Epoch 251/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.1510 - accuracy: 0.9550 - val_loss: 1.6658 - val_accuracy: 0.6867\n",
      "Epoch 252/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.1621 - accuracy: 0.9650 - val_loss: 1.6911 - val_accuracy: 0.6800\n",
      "Epoch 253/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1639 - accuracy: 0.9600 - val_loss: 1.6721 - val_accuracy: 0.6867\n",
      "Epoch 254/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.1781 - accuracy: 0.9500 - val_loss: 1.6801 - val_accuracy: 0.6733\n",
      "Epoch 255/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1138 - accuracy: 0.9667 - val_loss: 1.8596 - val_accuracy: 0.6733\n",
      "Epoch 256/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1677 - accuracy: 0.9567 - val_loss: 1.8136 - val_accuracy: 0.6733\n",
      "Epoch 257/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.1396 - accuracy: 0.9633 - val_loss: 1.7673 - val_accuracy: 0.6733\n",
      "Epoch 258/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.1107 - accuracy: 0.9700 - val_loss: 1.7482 - val_accuracy: 0.6600\n",
      "Epoch 259/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.1888 - accuracy: 0.9467 - val_loss: 1.7798 - val_accuracy: 0.6667\n",
      "Epoch 260/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.1707 - accuracy: 0.9483 - val_loss: 1.6479 - val_accuracy: 0.6933\n",
      "Epoch 261/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1335 - accuracy: 0.9667 - val_loss: 1.6014 - val_accuracy: 0.7200\n",
      "Epoch 262/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1178 - accuracy: 0.9667 - val_loss: 1.5496 - val_accuracy: 0.6933\n",
      "Epoch 263/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1446 - accuracy: 0.9633 - val_loss: 1.5254 - val_accuracy: 0.7267\n",
      "Epoch 264/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1215 - accuracy: 0.9583 - val_loss: 1.4708 - val_accuracy: 0.7267\n",
      "Epoch 265/500\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.93 - 0s 238us/step - loss: 0.2065 - accuracy: 0.9433 - val_loss: 1.5811 - val_accuracy: 0.6867\n",
      "Epoch 266/500\n",
      "600/600 [==============================] - 0s 250us/step - loss: 0.1045 - accuracy: 0.9750 - val_loss: 1.5621 - val_accuracy: 0.7067\n",
      "Epoch 267/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.0942 - accuracy: 0.9717 - val_loss: 1.6030 - val_accuracy: 0.6933\n",
      "Epoch 268/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.1218 - accuracy: 0.9717 - val_loss: 1.5719 - val_accuracy: 0.7067\n",
      "Epoch 269/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.0815 - accuracy: 0.9783 - val_loss: 1.5904 - val_accuracy: 0.7067\n",
      "Epoch 270/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1574 - accuracy: 0.9567 - val_loss: 1.5997 - val_accuracy: 0.6867\n",
      "Epoch 271/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.2287 - accuracy: 0.9467 - val_loss: 1.5989 - val_accuracy: 0.6600\n",
      "Epoch 272/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.1157 - accuracy: 0.9700 - val_loss: 1.4747 - val_accuracy: 0.7067\n",
      "Epoch 273/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.2364 - accuracy: 0.9383 - val_loss: 1.4875 - val_accuracy: 0.6867\n",
      "Epoch 274/500\n",
      "600/600 [==============================] - 0s 250us/step - loss: 0.2128 - accuracy: 0.9350 - val_loss: 1.5098 - val_accuracy: 0.6867\n",
      "Epoch 275/500\n",
      "600/600 [==============================] - 0s 259us/step - loss: 0.0916 - accuracy: 0.9767 - val_loss: 1.5537 - val_accuracy: 0.6667\n",
      "Epoch 276/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1435 - accuracy: 0.9517 - val_loss: 1.4381 - val_accuracy: 0.6800\n",
      "Epoch 277/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.0873 - accuracy: 0.9800 - val_loss: 1.5068 - val_accuracy: 0.6800\n",
      "Epoch 278/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.0648 - accuracy: 0.9783 - val_loss: 1.4977 - val_accuracy: 0.6800\n",
      "Epoch 279/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0689 - accuracy: 0.9800 - val_loss: 1.5342 - val_accuracy: 0.7067\n",
      "Epoch 280/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.1456 - accuracy: 0.9550 - val_loss: 1.5916 - val_accuracy: 0.7067\n",
      "Epoch 281/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1008 - accuracy: 0.9717 - val_loss: 1.6624 - val_accuracy: 0.6800\n",
      "Epoch 282/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1331 - accuracy: 0.9617 - val_loss: 1.6036 - val_accuracy: 0.7133\n",
      "Epoch 283/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.1790 - accuracy: 0.9433 - val_loss: 1.8439 - val_accuracy: 0.6800\n",
      "Epoch 284/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1756 - accuracy: 0.9467 - val_loss: 1.7475 - val_accuracy: 0.6867\n",
      "Epoch 285/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.1200 - accuracy: 0.9700 - val_loss: 1.6448 - val_accuracy: 0.6867\n",
      "Epoch 286/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.1407 - accuracy: 0.9600 - val_loss: 1.5969 - val_accuracy: 0.6733\n",
      "Epoch 287/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1327 - accuracy: 0.9667 - val_loss: 1.5380 - val_accuracy: 0.7000\n",
      "Epoch 288/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0919 - accuracy: 0.9700 - val_loss: 1.5524 - val_accuracy: 0.7000\n",
      "Epoch 289/500\n",
      "600/600 [==============================] - 0s 255us/step - loss: 0.1279 - accuracy: 0.9650 - val_loss: 1.5969 - val_accuracy: 0.6933\n",
      "Epoch 290/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.1376 - accuracy: 0.9600 - val_loss: 1.6500 - val_accuracy: 0.6933\n",
      "Epoch 291/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.1614 - accuracy: 0.9583 - val_loss: 1.6015 - val_accuracy: 0.6800\n",
      "Epoch 292/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.0963 - accuracy: 0.9683 - val_loss: 1.5781 - val_accuracy: 0.6933\n",
      "Epoch 293/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1037 - accuracy: 0.9717 - val_loss: 1.5894 - val_accuracy: 0.7067\n",
      "Epoch 294/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.2357 - accuracy: 0.9400 - val_loss: 1.5110 - val_accuracy: 0.6867\n",
      "Epoch 295/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1884 - accuracy: 0.9500 - val_loss: 1.5248 - val_accuracy: 0.7000\n",
      "Epoch 296/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.1686 - accuracy: 0.9467 - val_loss: 1.5758 - val_accuracy: 0.7133\n",
      "Epoch 297/500\n",
      "600/600 [==============================] - 0s 255us/step - loss: 0.1685 - accuracy: 0.9500 - val_loss: 1.6369 - val_accuracy: 0.6933\n",
      "Epoch 298/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.1303 - accuracy: 0.9683 - val_loss: 1.6476 - val_accuracy: 0.7067\n",
      "Epoch 299/500\n",
      "600/600 [==============================] - 0s 254us/step - loss: 0.1986 - accuracy: 0.9433 - val_loss: 1.6254 - val_accuracy: 0.7067\n",
      "Epoch 300/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0920 - accuracy: 0.9783 - val_loss: 1.4706 - val_accuracy: 0.7000\n",
      "Epoch 301/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1564 - accuracy: 0.9567 - val_loss: 1.6821 - val_accuracy: 0.6933\n",
      "Epoch 302/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.0884 - accuracy: 0.9800 - val_loss: 1.6778 - val_accuracy: 0.6733\n",
      "Epoch 303/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.0963 - accuracy: 0.9750 - val_loss: 1.6077 - val_accuracy: 0.6933\n",
      "Epoch 304/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.1203 - accuracy: 0.9683 - val_loss: 1.5115 - val_accuracy: 0.6933\n",
      "Epoch 305/500\n",
      "600/600 [==============================] - 0s 252us/step - loss: 0.1459 - accuracy: 0.9683 - val_loss: 1.5785 - val_accuracy: 0.7000\n",
      "Epoch 306/500\n",
      "600/600 [==============================] - 0s 259us/step - loss: 0.0723 - accuracy: 0.9750 - val_loss: 1.6427 - val_accuracy: 0.6867\n",
      "Epoch 307/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.1373 - accuracy: 0.9600 - val_loss: 1.5322 - val_accuracy: 0.7067\n",
      "Epoch 308/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1023 - accuracy: 0.9650 - val_loss: 1.5090 - val_accuracy: 0.7267\n",
      "Epoch 309/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1188 - accuracy: 0.9667 - val_loss: 1.4085 - val_accuracy: 0.7333\n",
      "Epoch 310/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.1539 - accuracy: 0.9700 - val_loss: 1.4624 - val_accuracy: 0.7333\n",
      "Epoch 311/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1038 - accuracy: 0.9733 - val_loss: 1.4764 - val_accuracy: 0.7267\n",
      "Epoch 312/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.1196 - accuracy: 0.9667 - val_loss: 1.4326 - val_accuracy: 0.7133\n",
      "Epoch 313/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.1245 - accuracy: 0.9650 - val_loss: 1.5155 - val_accuracy: 0.7200\n",
      "Epoch 314/500\n",
      "600/600 [==============================] - 0s 254us/step - loss: 0.0841 - accuracy: 0.9667 - val_loss: 1.6133 - val_accuracy: 0.6800\n",
      "Epoch 315/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.1180 - accuracy: 0.9650 - val_loss: 1.5910 - val_accuracy: 0.6867\n",
      "Epoch 316/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.1022 - accuracy: 0.9750 - val_loss: 1.5685 - val_accuracy: 0.7267\n",
      "Epoch 317/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.0645 - accuracy: 0.9817 - val_loss: 1.5696 - val_accuracy: 0.7267\n",
      "Epoch 318/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.1346 - accuracy: 0.9567 - val_loss: 1.6470 - val_accuracy: 0.7067\n",
      "Epoch 319/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.0914 - accuracy: 0.9700 - val_loss: 1.6603 - val_accuracy: 0.6600\n",
      "Epoch 320/500\n",
      "600/600 [==============================] - 0s 259us/step - loss: 0.1122 - accuracy: 0.9650 - val_loss: 1.6085 - val_accuracy: 0.6800\n",
      "Epoch 321/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.0744 - accuracy: 0.9783 - val_loss: 1.6368 - val_accuracy: 0.6867\n",
      "Epoch 322/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.0835 - accuracy: 0.9750 - val_loss: 1.5883 - val_accuracy: 0.6933\n",
      "Epoch 323/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0856 - accuracy: 0.9800 - val_loss: 1.6015 - val_accuracy: 0.6933\n",
      "Epoch 324/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1679 - accuracy: 0.9567 - val_loss: 1.5362 - val_accuracy: 0.7200\n",
      "Epoch 325/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.0917 - accuracy: 0.9750 - val_loss: 1.5893 - val_accuracy: 0.6933\n",
      "Epoch 326/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.0837 - accuracy: 0.9783 - val_loss: 1.6024 - val_accuracy: 0.7133\n",
      "Epoch 327/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.0708 - accuracy: 0.9750 - val_loss: 1.6071 - val_accuracy: 0.6933\n",
      "Epoch 328/500\n",
      "600/600 [==============================] - 0s 259us/step - loss: 0.1537 - accuracy: 0.9567 - val_loss: 1.5397 - val_accuracy: 0.7133\n",
      "Epoch 329/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.0986 - accuracy: 0.9733 - val_loss: 1.4733 - val_accuracy: 0.7067\n",
      "Epoch 330/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.0795 - accuracy: 0.9783 - val_loss: 1.6272 - val_accuracy: 0.7000\n",
      "Epoch 331/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1173 - accuracy: 0.9667 - val_loss: 1.7807 - val_accuracy: 0.6867\n",
      "Epoch 332/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1059 - accuracy: 0.9767 - val_loss: 1.7244 - val_accuracy: 0.7200\n",
      "Epoch 333/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.1386 - accuracy: 0.9617 - val_loss: 1.6501 - val_accuracy: 0.7200\n",
      "Epoch 334/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.0636 - accuracy: 0.9817 - val_loss: 1.5281 - val_accuracy: 0.7200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.0816 - accuracy: 0.9817 - val_loss: 1.4676 - val_accuracy: 0.7200\n",
      "Epoch 336/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.0786 - accuracy: 0.9767 - val_loss: 1.4822 - val_accuracy: 0.7267\n",
      "Epoch 337/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1154 - accuracy: 0.9633 - val_loss: 1.5215 - val_accuracy: 0.7000\n",
      "Epoch 338/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.1246 - accuracy: 0.9667 - val_loss: 1.5631 - val_accuracy: 0.7400\n",
      "Epoch 339/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1101 - accuracy: 0.9633 - val_loss: 1.6095 - val_accuracy: 0.7267\n",
      "Epoch 340/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.1468 - accuracy: 0.9567 - val_loss: 1.6374 - val_accuracy: 0.6933\n",
      "Epoch 341/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.0994 - accuracy: 0.9733 - val_loss: 1.6135 - val_accuracy: 0.7133\n",
      "Epoch 342/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.0616 - accuracy: 0.9783 - val_loss: 1.5842 - val_accuracy: 0.7467\n",
      "Epoch 343/500\n",
      "600/600 [==============================] - 0s 250us/step - loss: 0.1211 - accuracy: 0.9700 - val_loss: 1.6249 - val_accuracy: 0.7200\n",
      "Epoch 344/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.0997 - accuracy: 0.9817 - val_loss: 1.6815 - val_accuracy: 0.7067\n",
      "Epoch 345/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1021 - accuracy: 0.9700 - val_loss: 1.7464 - val_accuracy: 0.6800\n",
      "Epoch 346/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1270 - accuracy: 0.9633 - val_loss: 1.7001 - val_accuracy: 0.6867\n",
      "Epoch 347/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1864 - accuracy: 0.9500 - val_loss: 1.7584 - val_accuracy: 0.6733\n",
      "Epoch 348/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1573 - accuracy: 0.9617 - val_loss: 1.5815 - val_accuracy: 0.7067\n",
      "Epoch 349/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.0980 - accuracy: 0.9683 - val_loss: 1.5042 - val_accuracy: 0.7000\n",
      "Epoch 350/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.1192 - accuracy: 0.9633 - val_loss: 1.5476 - val_accuracy: 0.7067\n",
      "Epoch 351/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.1451 - accuracy: 0.9533 - val_loss: 1.5484 - val_accuracy: 0.7333\n",
      "Epoch 352/500\n",
      "600/600 [==============================] - 0s 252us/step - loss: 0.1196 - accuracy: 0.9650 - val_loss: 1.5474 - val_accuracy: 0.7267\n",
      "Epoch 353/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.1243 - accuracy: 0.9633 - val_loss: 1.6499 - val_accuracy: 0.7067\n",
      "Epoch 354/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.0833 - accuracy: 0.9767 - val_loss: 1.5992 - val_accuracy: 0.7000\n",
      "Epoch 355/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.0734 - accuracy: 0.9783 - val_loss: 1.5734 - val_accuracy: 0.7133\n",
      "Epoch 356/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.0851 - accuracy: 0.9750 - val_loss: 1.4458 - val_accuracy: 0.7000\n",
      "Epoch 357/500\n",
      "600/600 [==============================] - 0s 260us/step - loss: 0.1595 - accuracy: 0.9683 - val_loss: 1.4820 - val_accuracy: 0.7200\n",
      "Epoch 358/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1985 - accuracy: 0.9567 - val_loss: 1.5251 - val_accuracy: 0.6933\n",
      "Epoch 359/500\n",
      "600/600 [==============================] - 0s 252us/step - loss: 0.1413 - accuracy: 0.9583 - val_loss: 1.6390 - val_accuracy: 0.7133\n",
      "Epoch 360/500\n",
      "600/600 [==============================] - 0s 252us/step - loss: 0.1058 - accuracy: 0.9750 - val_loss: 1.5854 - val_accuracy: 0.7000\n",
      "Epoch 361/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.1008 - accuracy: 0.9717 - val_loss: 1.6157 - val_accuracy: 0.7333\n",
      "Epoch 362/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1088 - accuracy: 0.9750 - val_loss: 1.4529 - val_accuracy: 0.7333\n",
      "Epoch 363/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1577 - accuracy: 0.9583 - val_loss: 1.5881 - val_accuracy: 0.7067\n",
      "Epoch 364/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1012 - accuracy: 0.9750 - val_loss: 1.5630 - val_accuracy: 0.7400\n",
      "Epoch 365/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.0793 - accuracy: 0.9767 - val_loss: 1.5423 - val_accuracy: 0.7200\n",
      "Epoch 366/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.1177 - accuracy: 0.9683 - val_loss: 1.5388 - val_accuracy: 0.7067\n",
      "Epoch 367/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.1129 - accuracy: 0.9717 - val_loss: 1.5245 - val_accuracy: 0.7133\n",
      "Epoch 368/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.1069 - accuracy: 0.9767 - val_loss: 1.5408 - val_accuracy: 0.6933\n",
      "Epoch 369/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1058 - accuracy: 0.9717 - val_loss: 1.4869 - val_accuracy: 0.7067\n",
      "Epoch 370/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.0852 - accuracy: 0.9733 - val_loss: 1.6207 - val_accuracy: 0.7000\n",
      "Epoch 371/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1121 - accuracy: 0.9717 - val_loss: 1.6191 - val_accuracy: 0.6867\n",
      "Epoch 372/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.0778 - accuracy: 0.9767 - val_loss: 1.6330 - val_accuracy: 0.7200\n",
      "Epoch 373/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1439 - accuracy: 0.9583 - val_loss: 1.6516 - val_accuracy: 0.7133\n",
      "Epoch 374/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.1033 - accuracy: 0.9683 - val_loss: 1.7067 - val_accuracy: 0.6800\n",
      "Epoch 375/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.0722 - accuracy: 0.9817 - val_loss: 1.6747 - val_accuracy: 0.7133\n",
      "Epoch 376/500\n",
      "600/600 [==============================] - 0s 262us/step - loss: 0.0739 - accuracy: 0.9767 - val_loss: 1.6938 - val_accuracy: 0.7067\n",
      "Epoch 377/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.0828 - accuracy: 0.9800 - val_loss: 1.6833 - val_accuracy: 0.7067\n",
      "Epoch 378/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.0676 - accuracy: 0.9750 - val_loss: 1.7004 - val_accuracy: 0.7067\n",
      "Epoch 379/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.0630 - accuracy: 0.9833 - val_loss: 1.7129 - val_accuracy: 0.7400\n",
      "Epoch 380/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.1242 - accuracy: 0.9617 - val_loss: 1.7595 - val_accuracy: 0.7400\n",
      "Epoch 381/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.1218 - accuracy: 0.9683 - val_loss: 1.6258 - val_accuracy: 0.7333\n",
      "Epoch 382/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.0569 - accuracy: 0.9767 - val_loss: 1.6200 - val_accuracy: 0.7067\n",
      "Epoch 383/500\n",
      "600/600 [==============================] - 0s 254us/step - loss: 0.1186 - accuracy: 0.9733 - val_loss: 1.8729 - val_accuracy: 0.7133\n",
      "Epoch 384/500\n",
      "600/600 [==============================] - 0s 252us/step - loss: 0.0968 - accuracy: 0.9733 - val_loss: 1.9130 - val_accuracy: 0.7000\n",
      "Epoch 385/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.1007 - accuracy: 0.9700 - val_loss: 1.9125 - val_accuracy: 0.6733\n",
      "Epoch 386/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.0933 - accuracy: 0.9717 - val_loss: 1.8770 - val_accuracy: 0.6733\n",
      "Epoch 387/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.0709 - accuracy: 0.9850 - val_loss: 1.7913 - val_accuracy: 0.7133\n",
      "Epoch 388/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1167 - accuracy: 0.9733 - val_loss: 1.7077 - val_accuracy: 0.7000\n",
      "Epoch 389/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.0759 - accuracy: 0.9800 - val_loss: 1.7453 - val_accuracy: 0.6800\n",
      "Epoch 390/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1096 - accuracy: 0.9817 - val_loss: 1.7362 - val_accuracy: 0.7067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.0784 - accuracy: 0.9817 - val_loss: 1.8322 - val_accuracy: 0.7200\n",
      "Epoch 392/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0651 - accuracy: 0.9833 - val_loss: 1.8380 - val_accuracy: 0.7000\n",
      "Epoch 393/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.0859 - accuracy: 0.9750 - val_loss: 1.8621 - val_accuracy: 0.6867\n",
      "Epoch 394/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1226 - accuracy: 0.9717 - val_loss: 1.8390 - val_accuracy: 0.7067\n",
      "Epoch 395/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1328 - accuracy: 0.9700 - val_loss: 1.7794 - val_accuracy: 0.7067\n",
      "Epoch 396/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.0864 - accuracy: 0.9817 - val_loss: 1.7382 - val_accuracy: 0.6933\n",
      "Epoch 397/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.0995 - accuracy: 0.9667 - val_loss: 1.6851 - val_accuracy: 0.7000\n",
      "Epoch 398/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.0852 - accuracy: 0.9750 - val_loss: 1.7262 - val_accuracy: 0.6867\n",
      "Epoch 399/500\n",
      "600/600 [==============================] - 0s 264us/step - loss: 0.0808 - accuracy: 0.9783 - val_loss: 1.8165 - val_accuracy: 0.7000\n",
      "Epoch 400/500\n",
      "600/600 [==============================] - 0s 259us/step - loss: 0.1178 - accuracy: 0.9700 - val_loss: 1.8562 - val_accuracy: 0.6933\n",
      "Epoch 401/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.0979 - accuracy: 0.9783 - val_loss: 1.8636 - val_accuracy: 0.6667\n",
      "Epoch 402/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.0932 - accuracy: 0.9767 - val_loss: 1.8203 - val_accuracy: 0.6533\n",
      "Epoch 403/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.0950 - accuracy: 0.9817 - val_loss: 1.8236 - val_accuracy: 0.6867\n",
      "Epoch 404/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.0824 - accuracy: 0.9817 - val_loss: 1.8105 - val_accuracy: 0.6867\n",
      "Epoch 405/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.0798 - accuracy: 0.9800 - val_loss: 1.8825 - val_accuracy: 0.6933\n",
      "Epoch 406/500\n",
      "600/600 [==============================] - 0s 250us/step - loss: 0.1198 - accuracy: 0.9633 - val_loss: 1.7594 - val_accuracy: 0.7000\n",
      "Epoch 407/500\n",
      "600/600 [==============================] - 0s 255us/step - loss: 0.1527 - accuracy: 0.9550 - val_loss: 1.7663 - val_accuracy: 0.6933\n",
      "Epoch 408/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.0936 - accuracy: 0.9750 - val_loss: 1.6562 - val_accuracy: 0.6933\n",
      "Epoch 409/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.1146 - accuracy: 0.9717 - val_loss: 1.7134 - val_accuracy: 0.6933\n",
      "Epoch 410/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.0939 - accuracy: 0.9700 - val_loss: 1.6566 - val_accuracy: 0.6867\n",
      "Epoch 411/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 1.6838 - val_accuracy: 0.6867\n",
      "Epoch 412/500\n",
      "600/600 [==============================] - 0s 242us/step - loss: 0.1114 - accuracy: 0.9683 - val_loss: 1.7415 - val_accuracy: 0.6933\n",
      "Epoch 413/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.0846 - accuracy: 0.9767 - val_loss: 1.6574 - val_accuracy: 0.7133\n",
      "Epoch 414/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1193 - accuracy: 0.9600 - val_loss: 1.5389 - val_accuracy: 0.7267\n",
      "Epoch 415/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.1489 - accuracy: 0.9583 - val_loss: 1.6275 - val_accuracy: 0.7133\n",
      "Epoch 416/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1541 - accuracy: 0.9533 - val_loss: 1.6810 - val_accuracy: 0.6933\n",
      "Epoch 417/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1039 - accuracy: 0.9767 - val_loss: 1.7650 - val_accuracy: 0.6667\n",
      "Epoch 418/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.0542 - accuracy: 0.9800 - val_loss: 1.7905 - val_accuracy: 0.6733\n",
      "Epoch 419/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1207 - accuracy: 0.9800 - val_loss: 1.7133 - val_accuracy: 0.6867\n",
      "Epoch 420/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.0581 - accuracy: 0.9850 - val_loss: 1.6319 - val_accuracy: 0.7133\n",
      "Epoch 421/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.1039 - accuracy: 0.9683 - val_loss: 1.6568 - val_accuracy: 0.6933\n",
      "Epoch 422/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.0792 - accuracy: 0.9733 - val_loss: 1.5927 - val_accuracy: 0.6933\n",
      "Epoch 423/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.0872 - accuracy: 0.9733 - val_loss: 1.6420 - val_accuracy: 0.7333\n",
      "Epoch 424/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.0979 - accuracy: 0.9767 - val_loss: 1.5667 - val_accuracy: 0.7400\n",
      "Epoch 425/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1005 - accuracy: 0.9800 - val_loss: 1.5421 - val_accuracy: 0.7267\n",
      "Epoch 426/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.0355 - accuracy: 0.9900 - val_loss: 1.6076 - val_accuracy: 0.7133\n",
      "Epoch 427/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.0689 - accuracy: 0.9817 - val_loss: 1.6347 - val_accuracy: 0.7267\n",
      "Epoch 428/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.0981 - accuracy: 0.9717 - val_loss: 1.6745 - val_accuracy: 0.7000\n",
      "Epoch 429/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.0992 - accuracy: 0.9683 - val_loss: 1.6482 - val_accuracy: 0.7000\n",
      "Epoch 430/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.0903 - accuracy: 0.9750 - val_loss: 1.6110 - val_accuracy: 0.7133\n",
      "Epoch 431/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.0880 - accuracy: 0.9733 - val_loss: 1.6213 - val_accuracy: 0.7333\n",
      "Epoch 432/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1135 - accuracy: 0.9683 - val_loss: 1.5920 - val_accuracy: 0.7400\n",
      "Epoch 433/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.0953 - accuracy: 0.9733 - val_loss: 1.5494 - val_accuracy: 0.7267\n",
      "Epoch 434/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.0462 - accuracy: 0.9867 - val_loss: 1.5570 - val_accuracy: 0.7267\n",
      "Epoch 435/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.0637 - accuracy: 0.9817 - val_loss: 1.5461 - val_accuracy: 0.7333\n",
      "Epoch 436/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.0989 - accuracy: 0.9750 - val_loss: 1.5438 - val_accuracy: 0.7067\n",
      "Epoch 437/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.0598 - accuracy: 0.9883 - val_loss: 1.6009 - val_accuracy: 0.7200\n",
      "Epoch 438/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.0469 - accuracy: 0.9850 - val_loss: 1.6086 - val_accuracy: 0.7133\n",
      "Epoch 439/500\n",
      "600/600 [==============================] - 0s 247us/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 1.6479 - val_accuracy: 0.7200\n",
      "Epoch 440/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1225 - accuracy: 0.9683 - val_loss: 1.7782 - val_accuracy: 0.6867\n",
      "Epoch 441/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.0723 - accuracy: 0.9817 - val_loss: 1.8418 - val_accuracy: 0.7133\n",
      "Epoch 442/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.1131 - accuracy: 0.9650 - val_loss: 1.7359 - val_accuracy: 0.7133\n",
      "Epoch 443/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1092 - accuracy: 0.9600 - val_loss: 1.6537 - val_accuracy: 0.7200\n",
      "Epoch 444/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.0685 - accuracy: 0.9817 - val_loss: 1.8209 - val_accuracy: 0.6867\n",
      "Epoch 445/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.0964 - accuracy: 0.9617 - val_loss: 1.7790 - val_accuracy: 0.6733\n",
      "Epoch 446/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.0999 - accuracy: 0.9767 - val_loss: 1.8099 - val_accuracy: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 447/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.0646 - accuracy: 0.9817 - val_loss: 1.7674 - val_accuracy: 0.6933\n",
      "Epoch 448/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1288 - accuracy: 0.9700 - val_loss: 1.7107 - val_accuracy: 0.6867\n",
      "Epoch 449/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.1133 - accuracy: 0.9767 - val_loss: 1.5827 - val_accuracy: 0.7000\n",
      "Epoch 450/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.0810 - accuracy: 0.9767 - val_loss: 1.6703 - val_accuracy: 0.7200\n",
      "Epoch 451/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.2197 - accuracy: 0.9483 - val_loss: 1.6333 - val_accuracy: 0.7133\n",
      "Epoch 452/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.0848 - accuracy: 0.9800 - val_loss: 1.5649 - val_accuracy: 0.7067\n",
      "Epoch 453/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.0775 - accuracy: 0.9783 - val_loss: 1.5327 - val_accuracy: 0.7267\n",
      "Epoch 454/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.0919 - accuracy: 0.9783 - val_loss: 1.5845 - val_accuracy: 0.6867\n",
      "Epoch 455/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0608 - accuracy: 0.9850 - val_loss: 1.5625 - val_accuracy: 0.7067\n",
      "Epoch 456/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.0731 - accuracy: 0.9833 - val_loss: 1.5293 - val_accuracy: 0.7133\n",
      "Epoch 457/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.0817 - accuracy: 0.9833 - val_loss: 1.4848 - val_accuracy: 0.7333\n",
      "Epoch 458/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.0795 - accuracy: 0.9833 - val_loss: 1.5091 - val_accuracy: 0.7067\n",
      "Epoch 459/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.1576 - accuracy: 0.9667 - val_loss: 1.5526 - val_accuracy: 0.6667\n",
      "Epoch 460/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.1023 - accuracy: 0.9750 - val_loss: 1.5768 - val_accuracy: 0.6867\n",
      "Epoch 461/500\n",
      "600/600 [==============================] - 0s 237us/step - loss: 0.1383 - accuracy: 0.9567 - val_loss: 1.6798 - val_accuracy: 0.7000\n",
      "Epoch 462/500\n",
      "600/600 [==============================] - 0s 249us/step - loss: 0.0923 - accuracy: 0.9667 - val_loss: 1.5697 - val_accuracy: 0.6933\n",
      "Epoch 463/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.1388 - accuracy: 0.9633 - val_loss: 1.6152 - val_accuracy: 0.6533\n",
      "Epoch 464/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1153 - accuracy: 0.9683 - val_loss: 1.5723 - val_accuracy: 0.6867\n",
      "Epoch 465/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.0656 - accuracy: 0.9800 - val_loss: 1.5567 - val_accuracy: 0.6800\n",
      "Epoch 466/500\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.0749 - accuracy: 0.9850 - val_loss: 1.4907 - val_accuracy: 0.7200\n",
      "Epoch 467/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.1222 - accuracy: 0.9700 - val_loss: 1.4072 - val_accuracy: 0.7133\n",
      "Epoch 468/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.0605 - accuracy: 0.9867 - val_loss: 1.5636 - val_accuracy: 0.6800\n",
      "Epoch 469/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.0768 - accuracy: 0.9817 - val_loss: 1.6594 - val_accuracy: 0.6733\n",
      "Epoch 470/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1519 - accuracy: 0.9667 - val_loss: 1.6646 - val_accuracy: 0.6867\n",
      "Epoch 471/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.0595 - accuracy: 0.9833 - val_loss: 1.7210 - val_accuracy: 0.6867\n",
      "Epoch 472/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.1286 - accuracy: 0.9600 - val_loss: 1.6868 - val_accuracy: 0.6933\n",
      "Epoch 473/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0940 - accuracy: 0.9683 - val_loss: 1.7121 - val_accuracy: 0.6933\n",
      "Epoch 474/500\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.1131 - accuracy: 0.9750 - val_loss: 1.5443 - val_accuracy: 0.7067\n",
      "Epoch 475/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.0892 - accuracy: 0.9733 - val_loss: 1.5882 - val_accuracy: 0.7000\n",
      "Epoch 476/500\n",
      "600/600 [==============================] - 0s 250us/step - loss: 0.1195 - accuracy: 0.9650 - val_loss: 1.5265 - val_accuracy: 0.7000\n",
      "Epoch 477/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.0736 - accuracy: 0.9767 - val_loss: 1.4980 - val_accuracy: 0.7133\n",
      "Epoch 478/500\n",
      "600/600 [==============================] - 0s 253us/step - loss: 0.0848 - accuracy: 0.9767 - val_loss: 1.5026 - val_accuracy: 0.7200\n",
      "Epoch 479/500\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.0314 - accuracy: 0.9883 - val_loss: 1.5393 - val_accuracy: 0.7267\n",
      "Epoch 480/500\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.0620 - accuracy: 0.9833 - val_loss: 1.6129 - val_accuracy: 0.7000\n",
      "Epoch 481/500\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.0468 - accuracy: 0.9883 - val_loss: 1.6786 - val_accuracy: 0.7000\n",
      "Epoch 482/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0741 - accuracy: 0.9883 - val_loss: 1.7334 - val_accuracy: 0.7267\n",
      "Epoch 483/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0914 - accuracy: 0.9750 - val_loss: 1.7519 - val_accuracy: 0.7267\n",
      "Epoch 484/500\n",
      "600/600 [==============================] - 0s 248us/step - loss: 0.1142 - accuracy: 0.9667 - val_loss: 1.6418 - val_accuracy: 0.7267\n",
      "Epoch 485/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0838 - accuracy: 0.9850 - val_loss: 1.7032 - val_accuracy: 0.7133\n",
      "Epoch 486/500\n",
      "600/600 [==============================] - 0s 250us/step - loss: 0.0971 - accuracy: 0.9700 - val_loss: 1.6749 - val_accuracy: 0.6867\n",
      "Epoch 487/500\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.0770 - accuracy: 0.9783 - val_loss: 1.7867 - val_accuracy: 0.7067\n",
      "Epoch 488/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.0988 - accuracy: 0.9667 - val_loss: 1.7852 - val_accuracy: 0.6933\n",
      "Epoch 489/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0571 - accuracy: 0.9867 - val_loss: 1.8403 - val_accuracy: 0.6733\n",
      "Epoch 490/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0503 - accuracy: 0.9883 - val_loss: 1.8626 - val_accuracy: 0.6733\n",
      "Epoch 491/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.0486 - accuracy: 0.9867 - val_loss: 1.9061 - val_accuracy: 0.6933\n",
      "Epoch 492/500\n",
      "600/600 [==============================] - 0s 243us/step - loss: 0.1211 - accuracy: 0.9717 - val_loss: 1.9823 - val_accuracy: 0.6933\n",
      "Epoch 493/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.1039 - accuracy: 0.9650 - val_loss: 1.9772 - val_accuracy: 0.6933\n",
      "Epoch 494/500\n",
      "600/600 [==============================] - 0s 240us/step - loss: 0.0815 - accuracy: 0.9833 - val_loss: 1.9533 - val_accuracy: 0.7000\n",
      "Epoch 495/500\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.0948 - accuracy: 0.9717 - val_loss: 1.9075 - val_accuracy: 0.6867\n",
      "Epoch 496/500\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 1.8247 - val_accuracy: 0.6867\n",
      "Epoch 497/500\n",
      "600/600 [==============================] - 0s 238us/step - loss: 0.0898 - accuracy: 0.9717 - val_loss: 1.7635 - val_accuracy: 0.6867\n",
      "Epoch 498/500\n",
      "600/600 [==============================] - 0s 233us/step - loss: 0.1197 - accuracy: 0.9650 - val_loss: 1.7986 - val_accuracy: 0.7000\n",
      "Epoch 499/500\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.0960 - accuracy: 0.9733 - val_loss: 1.8593 - val_accuracy: 0.7200\n",
      "Epoch 500/500\n",
      "600/600 [==============================] - 0s 245us/step - loss: 0.0977 - accuracy: 0.9700 - val_loss: 1.7984 - val_accuracy: 0.7133\n"
     ]
    }
   ],
   "source": [
    "training_res = model.fit(x_train, y_train, batch_size = 32, epochs = 500, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.set_ylim([0.0, 1.0])\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "loss_ax.plot(training_res.history['loss'], 'y', label='training loss')\n",
    "acc_ax.plot(training_res.history['accuracy'], 'b', label='training accuracy')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = y_pred.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_abs = []\n",
    "maxx = 0\n",
    "t = 0\n",
    "for i in range(0, y_pred.shape[0]):\n",
    "    for j in range(0, y_pred.shape[1]):\n",
    "        if y_pred[i][j] >= maxx:\n",
    "            maxx = y_pred[i][j]\n",
    "            t = j\n",
    "    y_pred_abs.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 33]\n",
      " [ 1 34]\n",
      " [ 2 26]\n",
      " [ 3 15]\n",
      " [ 4 22]\n",
      " [ 5 20]\n",
      " [ 6 27]\n",
      " [ 7 30]\n",
      " [ 8 25]\n",
      " [ 9 18]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_pred_abs, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.85      0.75        26\n",
      "           1       0.88      1.00      0.94        30\n",
      "           2       0.69      0.58      0.63        31\n",
      "           3       0.67      0.48      0.56        21\n",
      "           4       0.64      0.82      0.72        17\n",
      "           5       0.75      0.83      0.79        18\n",
      "           6       0.81      0.92      0.86        24\n",
      "           7       0.97      0.91      0.94        32\n",
      "           8       0.72      0.82      0.77        22\n",
      "           9       0.78      0.48      0.60        29\n",
      "\n",
      "    accuracy                           0.77       250\n",
      "   macro avg       0.76      0.77      0.75       250\n",
      "weighted avg       0.77      0.77      0.76       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(y_test, y_pred_abs, target_names = target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
